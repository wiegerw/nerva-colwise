<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.23">
<meta name="description" content="Documentation for the nerva-colwise repository.">
<meta name="author" content="Wieger Wesselink">
<meta name="copyright" content="Copyright 2024 Wieger Wesselink">
<title>Nerva-rowwise C++ manual</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/*! Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment the following line when using as a custom stylesheet */
/* @import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"; */
html{font-family:sans-serif;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
b,strong{font-weight:bold}
abbr{font-size:.9em}
abbr[title]{cursor:help;border-bottom:1px dotted #dddddf;text-decoration:none}
dfn{font-style:italic}
hr{height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type=checkbox],input[type=radio]{padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,::before,::after{box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;line-height:1;position:relative;cursor:auto;-moz-tab-size:4;-o-tab-size:4;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ul.square{list-style-type:square}
ul.circle ul:not([class]),ul.disc ul:not([class]),ul.square ul:not([class]){list-style:inherit}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:1px solid #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;border-radius:3px;box-shadow:0 1px 0 rgba(0,0,0,.2),inset 0 0 0 .1em #fff;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin:0 auto;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child{border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:flex;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border:1px solid #e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:hsla(0,0%,100%,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details{margin-left:1.25rem}
details>summary{cursor:pointer;display:block;position:relative;line-height:1.6;margin-bottom:.625rem;outline:none;-webkit-tap-highlight-color:transparent}
details>summary::-webkit-details-marker{display:none}
details>summary::before{content:"";border:solid transparent;border-left:solid;border-width:.3em 0 .3em .5em;position:absolute;top:.5em;left:-1.25rem;transform:translateX(15%)}
details[open]>summary::before{border:solid transparent;border-top:solid;border-width:.5em .3em 0;transform:translateY(15%)}
details>summary::after{content:"";width:1.25rem;height:1em;position:absolute;top:.3em;left:-1.25rem}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class=paragraph]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border:1px solid #e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;border-radius:4px}
.sidebarblock{border:1px solid #dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;border-radius:4px}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:first-child,.sidebarblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child,.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class=highlight],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos,pre.pygments .linenos{border-right:1px solid;opacity:.35;padding-right:.5em;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
pre.pygments span.linenos{display:inline-block;margin-right:.75em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans-serif;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all>*>tr,table.stripes-odd>*>tr:nth-of-type(odd),table.stripes-even>*>tr:nth-of-type(even),table.stripes-hover>*>tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
li>p:empty:only-child::before{content:"";display:inline-block}
ul.checklist>li>p:first-child{margin-left:-1em}
ul.checklist>li>p:first-child>.fa-square-o:first-child,ul.checklist>li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist>li>p:first-child>input[type=checkbox]:first-child{margin-right:.25em}
ul.inline{display:flex;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:4px solid #fff;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active,#footnotes .footnote a:first-of-type:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt,summary{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,td.hdlist1,span.alt,summary{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]{border-bottom:1px dotted}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media amzn-kf8,print{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<style>
pre.rouge table td { padding: 5px; }
pre.rouge table pre { margin: 0; }
pre.rouge, pre.rouge .w {
  color: #24292f;
  background-color: #f6f8fa;
}
pre.rouge .k, pre.rouge .kd, pre.rouge .kn, pre.rouge .kp, pre.rouge .kr, pre.rouge .kt, pre.rouge .kv {
  color: #cf222e;
}
pre.rouge .gr {
  color: #f6f8fa;
}
pre.rouge .gd {
  color: #82071e;
  background-color: #ffebe9;
}
pre.rouge .nb {
  color: #953800;
}
pre.rouge .nc {
  color: #953800;
}
pre.rouge .no {
  color: #953800;
}
pre.rouge .nn {
  color: #953800;
}
pre.rouge .sr {
  color: #116329;
}
pre.rouge .na {
  color: #116329;
}
pre.rouge .nt {
  color: #116329;
}
pre.rouge .gi {
  color: #116329;
  background-color: #dafbe1;
}
pre.rouge .ges {
  font-weight: bold;
  font-style: italic;
}
pre.rouge .kc {
  color: #0550ae;
}
pre.rouge .l, pre.rouge .ld, pre.rouge .m, pre.rouge .mb, pre.rouge .mf, pre.rouge .mh, pre.rouge .mi, pre.rouge .il, pre.rouge .mo, pre.rouge .mx {
  color: #0550ae;
}
pre.rouge .sb {
  color: #0550ae;
}
pre.rouge .bp {
  color: #0550ae;
}
pre.rouge .ne {
  color: #0550ae;
}
pre.rouge .nl {
  color: #0550ae;
}
pre.rouge .py {
  color: #0550ae;
}
pre.rouge .nv, pre.rouge .vc, pre.rouge .vg, pre.rouge .vi, pre.rouge .vm {
  color: #0550ae;
}
pre.rouge .o, pre.rouge .ow {
  color: #0550ae;
}
pre.rouge .gh {
  color: #0550ae;
  font-weight: bold;
}
pre.rouge .gu {
  color: #0550ae;
  font-weight: bold;
}
pre.rouge .s, pre.rouge .sa, pre.rouge .sc, pre.rouge .dl, pre.rouge .sd, pre.rouge .s2, pre.rouge .se, pre.rouge .sh, pre.rouge .sx, pre.rouge .s1, pre.rouge .ss {
  color: #0a3069;
}
pre.rouge .nd {
  color: #8250df;
}
pre.rouge .nf, pre.rouge .fm {
  color: #8250df;
}
pre.rouge .err {
  color: #f6f8fa;
  background-color: #82071e;
}
pre.rouge .c, pre.rouge .ch, pre.rouge .cd, pre.rouge .cm, pre.rouge .cp, pre.rouge .cpf, pre.rouge .c1, pre.rouge .cs {
  color: #6e7781;
}
pre.rouge .gl {
  color: #6e7781;
}
pre.rouge .gt {
  color: #6e7781;
}
pre.rouge .ni {
  color: #24292f;
}
pre.rouge .si {
  color: #24292f;
}
pre.rouge .ge {
  color: #24292f;
  font-style: italic;
}
pre.rouge .gs {
  color: #24292f;
  font-weight: bold;
}
</style>
</head>
<body id="demo" class="book toc2 toc-left">
<div id="header">
<h1>Nerva-rowwise C++ manual</h1>
<div class="details">
<span id="author" class="author">Wieger Wesselink</span><br>
<span id="email" class="email"><a href="mailto:j.w.wesselink@tue.nl">j.w.wesselink@tue.nl</a></span><br>
</div>
<div id="toc" class="toc2">
<div id="toctitle"><h3>Contents</h3></div>
<ul class="sectlevel1">
<li><a href="#_introduction">Introduction</a>
<ul class="sectlevel2">
<li><a href="#_installation">Installation</a></li>
</ul>
</li>
<li><a href="#_command_line_tools">Command line tools</a>
<ul class="sectlevel2">
<li><a href="#_the_tool_mlp">The tool mlp</a></li>
<li><a href="#_the_tool_mkl">The tool mkl</a></li>
<li><a href="#_the_tool_inspect_npz">The tool inspect_npz</a></li>
</ul>
</li>
<li><a href="#_overview_of_the_code">Overview of the code</a>
<ul class="sectlevel2">
<li><a href="#_number_type">Number type</a></li>
<li><a href="#_header_files">Header files</a></li>
<li><a href="#_classes">Classes</a></li>
<li><a href="#_training_a_neural_network">Training a neural network</a></li>
<li><a href="#_timers">Timers</a></li>
</ul>
</li>
<li><a href="#_matrix_operations">Matrix operations</a>
<ul class="sectlevel2">
<li><a href="#_eigen_library">Eigen library</a></li>
<li><a href="#_mkl_library">MKL library</a></li>
</ul>
</li>
<li><a href="#io">I/O</a></li>
<li><a href="#_performance">Performance</a>
<ul class="sectlevel2">
<li><a href="#_mini_batches">Mini-batches</a></li>
<li><a href="#_matrix_products">Matrix products</a></li>
<li><a href="#_nerva_computation_mode">Nerva computation mode</a></li>
<li><a href="#_subnormal_numbers">Subnormal numbers</a></li>
</ul>
</li>
<li><a href="#_sparse_neural_networks">Sparse neural networks</a>
<ul class="sectlevel2">
<li><a href="#_sparse_matrices">Sparse matrices</a></li>
<li><a href="#_sparse_evolutionary_training">Sparse evolutionary training</a></li>
<li><a href="#_sparse_initialization">Sparse initialization</a></li>
<li><a href="#_pruning_weights">Pruning weights</a></li>
<li><a href="#_growing_weights">Growing weights</a></li>
<li><a href="#_classes_for_pruning_and_growing">Classes for pruning and growing</a></li>
<li><a href="#_experiments_with_sparse_training">Experiments with sparse training</a></li>
</ul>
</li>
<li><a href="#extending">Extending the library</a></li>
<li><a href="#_references">References</a></li>
</ul>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<style>
  .small-code .content pre {
      font-size: 0.7em;
  }
</style>
</div>
</div>
<div class="sect1">
<h2 id="_introduction">Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This document describes the implementation of the <em>Nerva-Colwise C++ Library</em>. In earlier versions, I relied heavily on lecture notes from <a href="https://www.cs.toronto.edu/~rgrosse/teaching.html">machine learning courses by Roger Grosse</a>, which I highly recommend. This influence may still be evident in the naming of symbols.</p>
</div>
<div class="sect2">
<h3 id="_installation">Installation</h3>
<div class="paragraph">
<p>The following build systems are supported:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://cmake.org/">CMake</a> 3.16+</p>
</li>
<li>
<p><a href="https://www.bfgroup.xyz/b2/">B2</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Using CMake, the <em>Nerva-Colwise C++ Library</em> can be built in a standard way. The only prerequisite is that the Intel OneAPI library must be installed, and two environment variables must be set. The variable <code>ONEAPI_ROOT</code> should point to the OneAPI installation directory, and the variable <code>MKL_DIR</code> to the MKL installation directory. The dependencies for doctest, Eigen, FMT, Lyra and pybind11 are resolved automatically using <code>FetchContent</code> commands.</p>
</div>
<div class="sect3">
<h4 id="_linux_install">Linux install</h4>
<div class="paragraph">
<p>The build and install can for example be done like this on a Linux system:</p>
</div>
<div id="cmake-build" class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="bash"><span class="nb">mkdir </span>build
<span class="nb">cd </span>build
cmake .. <span class="se">\</span>
    <span class="nt">-DCMAKE_INSTALL_PREFIX</span><span class="o">=</span>../install <span class="se">\</span>
    <span class="nt">-DCMAKE_BUILD_TYPE</span><span class="o">=</span>RELEASE <span class="se">\</span>
    <span class="nt">-DMKL_DIR</span>:PATH<span class="o">=</span><span class="nv">$ONEAPI_ROOT</span>/mkl/latest/lib/cmake/mkl
make <span class="nt">-j8</span>
make <span class="nb">install</span></code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
Setting the <code>MKL_DIR</code> variable is not necessary if the OneAPI <code>setvars.sh</code> script is run beforehand.
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
For the clang and icpx compilers one should know that there is a longstanding issue between MKL and Eigen. See
<a href="https://community.intel.com/t5/Intel-oneAPI-Math-Kernel-Library/Using-MKL-2023-0-0-20221201-with-Eigen/m-p/1456044">1456044</a>
and <a href="https://gitlab.com/libeigen/eigen/-/issues/2586">issues/2586</a> for more details.
To resolve this issue, a hack has been applied in Nerva: by defining the symbol <code>EIGEN_COLPIVOTINGHOUSEHOLDERQR_LAPACKE_H</code>,
the inclusion of the offending header file is prevented.
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>For the B2 build using the icpx compiler, the environment needs to be changed like this in advance. Alternatively, the OneAPI <code>setvars.sh</code> script can be called.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$ONEAPI_ROOT/latest/lib
PATH=$PATH:/$ONEAPI_ROOT/latest/bin</pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_sycl_install">SYCL install</h4>
<div class="paragraph">
<p>There is an experimental computation mode for SYCL matrix operations, that can be enabled in the <code>mlp</code> tool using the flag <code>--computation=sycl</code>. This is only supported on Linux in combination with the Intel <code>icpx</code> compiler. It can be enabled using the following build command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code>source $ONEAPI_ROOT/setvars.sh
cmake .. \
-DCMAKE_C_COMPILER=$ONEAPI_ROOT/compiler/latest/bin/icx \
-DCMAKE_CXX_COMPILER=$ONEAPI_ROOT/compiler/latest/bin/icpx \
-DCMAKE_INSTALL_PREFIX=../install \
-DCMAKE_BUILD_TYPE=RELEASE \
-DENABLE_SYCL=ON</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The initial tests with SYCL matrix operations on CPU have a rather disappointing performance.
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_windows_visual_studio_install">Windows Visual Studio install</h4>
<div class="paragraph">
<p>On Windows, a Visual Studio command line build can be done like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code>cmake .. ^
    -G "NMake Makefiles" ^
    -DCMAKE_INSTALL_PREFIX=..\install ^
    -DCMAKE_BUILD_TYPE=Release ^
    -DMKL_DIR="%ONEAPI_ROOT%\latest\lib\cmake\mkl"
nmake
nmake install</code></pre>
</div>
</div>
<div class="paragraph">
<p>The unit tests can be run using the command</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code>ctest -R nerva</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
The number of threads that are used at runtime by the MKL library can be controlled using the environment variables <code>MKL_NUM_THREADS</code> and <code>OMP_NUM_THREADS</code>, see also <a href="https://www.intel.com/content/www/us/en/docs/onemkl/developer-guide-linux/2024-2/techniques-to-set-the-number-of-threads.html">techniques-to-set-the-number-of-threads.html</a>.
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_command_line_tools">Command line tools</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The following command line tools are available. They can be found in the <code>tools</code> directory.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Tool</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>mlp</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A tool for training multilayer perceptrons.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>mkl</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A tool for benchmarking sparse and dense matrix products using the Intel MKL library.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>inspect_npz</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A tool for inspecting the contents of a file in NumPy NPZ format.</p></td>
</tr>
</tbody>
</table>
<div class="sect2">
<h3 id="_the_tool_mlp">The tool mlp</h3>
<div class="paragraph">
<p>The tool <code>mlp</code> can be used for training multilayer perceptrons. An example invocation of the <code>mlp</code> tool is</p>
</div>
<div class="listingblock small-code">
<div class="content">
<pre class="rouge highlight"><code data-lang="bash">../install/bin/mlp <span class="se">\</span>
    <span class="nt">--layers</span><span class="o">=</span><span class="s2">"ReLU;ReLU;Linear"</span> <span class="se">\</span>
    <span class="nt">--layer-sizes</span><span class="o">=</span><span class="s2">"3072;1024;1024;10"</span> <span class="se">\</span>
    <span class="nt">--layer-weights</span><span class="o">=</span>Xavier <span class="se">\</span>
    <span class="nt">--optimizers</span><span class="o">=</span><span class="s2">"Nesterov(0.9)"</span> <span class="se">\</span>
    <span class="nt">--loss</span><span class="o">=</span>SoftmaxCrossEntropy <span class="se">\</span>
    <span class="nt">--learning-rate</span><span class="o">=</span>0.01 <span class="se">\</span>
    <span class="nt">--epochs</span><span class="o">=</span>100 <span class="se">\</span>
    <span class="nt">--batch-size</span><span class="o">=</span>100 <span class="se">\</span>
    <span class="nt">--threads</span><span class="o">=</span>12 <span class="se">\</span>
    <span class="nt">--overall-density</span><span class="o">=</span>0.05 <span class="se">\</span>
    <span class="nt">--cifar10</span><span class="o">=</span>../data <span class="se">\</span>
    <span class="nt">--seed</span><span class="o">=</span>123</code></pre>
</div>
</div>
<div class="paragraph">
<p>This will train a CIFAR-10 model using an MLP consisting of three layers with activation functions ReLU, ReLU and no activation. Note that first the CIFAR-10 binary version needs to be downloaded from <a href="https://www.cs.toronto.edu/~kriz/cifar.html" class="bare">https://www.cs.toronto.edu/~kriz/cifar.html</a>.</p>
</div>
<div class="paragraph">
<p>The output may look like this:</p>
</div>
<div id="mlp_output" class="listingblock small-code">
<div class="content">
<pre>=== Nerva c++ model ===
Sparse(input_size=3072, output_size=1024, density=0.042382877, optimizer=Nesterov(0.90000), activation=ReLU())
Sparse(input_size=1024, output_size=1024, density=0.06357384, optimizer=Nesterov(0.90000), activation=ReLU())
Dense(input_size=1024, output_size=10, optimizer=Nesterov(0.90000), activation=NoActivation())
loss = SoftmaxCrossEntropyLoss()
scheduler = ConstantScheduler(lr=0.01)
layer densities: 133325/3145728 (4.238%), 66662/1048576 (6.357%), 10240/10240 (100%)

epoch   0 lr: 0.01000000  loss: 2.30284437  train accuracy: 0.07904000  test accuracy: 0.08060000 time: 0.00000000s
epoch   1 lr: 0.01000000  loss: 2.14723837  train accuracy: 0.21136000  test accuracy: 0.21320000 time: 2.74594253s
epoch   2 lr: 0.01000000  loss: 1.91454245  train accuracy: 0.29976000  test accuracy: 0.29940000 time: 2.76982510s
epoch   3 lr: 0.01000000  loss: 1.78019225  train accuracy: 0.35416000  test accuracy: 0.35820000 time: 2.69554319s
epoch   4 lr: 0.01000000  loss: 1.68071066  train accuracy: 0.39838000  test accuracy: 0.40000000 time: 2.68532307s
epoch   5 lr: 0.01000000  loss: 1.59761505  train accuracy: 0.42820000  test accuracy: 0.43060000 time: 3.02131606s</pre>
</div>
</div>
<div class="sect3">
<h4 id="mlp_tool">mlp command line options</h4>
<div class="paragraph">
<p>This section gives an overview of the command line interface of the <code>mlp</code> tool.</p>
</div>
<div class="sect4">
<h5 id="_parameters_lists">Parameters lists</h5>
<div class="paragraph">
<p>Some command line options take a list of items as input, for example a list of layers. These items must be separated by semicolons, e.g. <code>--layers="ReLU;ReLU;Linear"</code>.</p>
</div>
</div>
<div class="sect4">
<h5 id="_named_parameters">Named parameters</h5>
<div class="paragraph">
<p>Some of the items take parameters. For this we use a function call syntax with named parameters, e.g. <code>AllReLU(alpha=0.3)</code>. In case that there is only one parameter, the name may be omitted: <code>AllReLU(0.3)</code>. If the parameters have default values, they may be omitted. For example, <code>TReLU</code> or <code>TReLU()</code> is equivalent to <code>TReLU(al=0,tl=0,ar=0,tr=1)</code>.</p>
</div>
</div>
<div class="sect4">
<h5 id="_general_options">General options</h5>
<div class="ulist">
<ul>
<li>
<p><code>-?</code>, <code>-h</code>, <code>--help</code>
Display help information.</p>
</li>
<li>
<p><code>--debug</code>, <code>-d</code>
Show debug output. This prints batches, weight matrices, bias vectors, gradients etc.</p>
</li>
<li>
<p><code>--verbose</code>, <code>-v</code>
Show verbose output.</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_random_generator_options">Random generator options</h5>
<div class="ulist">
<ul>
<li>
<p><code>--seed &lt;value&gt;</code>
A seed value for the random generator.</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_layer_configuration_options">Layer configuration options</h5>
<div class="ulist">
<ul>
<li>
<p><code>--layers &lt;value&gt;</code>
A semicolon separated list of layers. For example, <code>--layers=ReLU;AllReLU(0.3);Linear</code> is used to specify a neural network with three layers with an ReLU, AllReLU and no activation function. The following layers are supported:</p>
</li>
</ul>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Specification</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Linear</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Linear layer without activation</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>ReLU</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Linear layer with ReLU activation</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Sigmoid</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Linear layer with sigmoid activation</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Softmax</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Linear layer with softmax activation</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>LogSoftmax</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Linear layer with log-softmax activation</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>HyperbolicTangent</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Linear layer with hyperbolic tangent activation</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>AllReLU(&lt;alpha&gt;)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Linear layer with All ReLU activation</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>SReLU(&lt;al&gt;,&lt;tl&gt;,&lt;ar&gt;,&lt;tr&gt;)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Linear layer with SReLU activation. The default value for the parameters are <code>al=0, tl=0, ar=0, tr=1</code>. For these
 values <code>SReLU</code> coincides with <code>ReLU</code>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>TReLU(&lt;epsilon&gt;)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Linear layer with trimmed ReLU activation</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>BatchNormalization</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Batch normalization layer</p></td>
</tr>
</tbody>
</table>
<div class="ulist">
<ul>
<li>
<p><code>--layer-sizes &lt;value&gt;</code>
A semicolon-separated list of the sizes of linear layers of the multilayer perceptron. For example, <code>--layer-sizes=3072;1024;512;10</code> specifies the sizes of three linear layers. The first one has 3072 inputs and 1024 outputs, the second one 1024 inputs and 512 outputs, and the third one has 512 inputs and 10 outputs.</p>
</li>
<li>
<p><code>--densities &lt;value&gt;</code>
A comma-separated list of linear layer densities. By default, all linear layers are dense (i.e. have density 1.0). If only one value is
 specified, it will be used for all linear layers.</p>
</li>
<li>
<p><code>--dropouts &lt;value&gt;</code>
A comma-separated list of dropout rates of linear layers. By default, all linear layers have no dropout (i.e. dropout rate 0.0).</p>
</li>
<li>
<p><code>--overall-density &lt;value&gt;</code>
The overall density of the linear layers. This value should be in the interval \([0,1\)], and it specifies the fraction of the total number of weights that is non-zero. The overall density is not distributed evenly over the layers. Instead, small layers will be assigned a higher density than large layers.</p>
</li>
<li>
<p><code>--layer-weights &lt;value&gt;</code>
The generator that is used for initializing the weights of the linear layers. The following weight generators are supported:</p>
</li>
</ul>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Specification</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Xavier</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Xavier weights</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>XavierNormalized</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Normalized Xavier weights</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>He</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Kaiming He weights</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Uniform</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Uniform weights</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Zero</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">All weights are zero (N.B. This usually doesn&#8217;t work)</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect4">
<h5 id="_training_configuration_options">Training configuration options</h5>
<div class="ulist">
<ul>
<li>
<p><code>--epochs &lt;value&gt;</code>
The number of epochs of the training (default: 100).</p>
</li>
<li>
<p><code>--batch-size &lt;value&gt;</code>
The batch size of the training.</p>
</li>
<li>
<p><code>--no-shuffle</code>
Do not shuffle the dataset during training.</p>
</li>
<li>
<p><code>--no-statistics</code>
Do not display intermediate statistics during training.</p>
</li>
<li>
<p><code>--optimizers &lt;value&gt;</code>
A semicolon-separated list of optimizers used for linear and batch normalization layers. The following optimizers are supported:</p>
</li>
</ul>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Specification</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>GradientDescent</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Gradient descent optimization</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Momentum(mu)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Momentum optimization with momentum parameter <code>mu</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Nesterov(mu)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Nesterov optimization with momentum parameter <code>mu</code></p></td>
</tr>
</tbody>
</table>
<div class="ulist">
<ul>
<li>
<p><code>--learning-rate &lt;value&gt;</code>
A semicolon-separated list of learning rate schedulers of linear and batch normalization layers. If only one learning rate scheduler is specified, it is applied to all layers. The following learning rate schedulers are supported:</p>
</li>
</ul>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Specification</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Constant(lr)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Constant learning rate <code>lr</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>TimeBased(lr, decay)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Adaptive learning rate with decay</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>StepBased(lr, drop_rate, change_rate)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Step based learning rate where the learning rate is regularly dropped
to a lower value</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>MultistepLR(lr, milestones, gamma)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Step based learning rate, where <code>milestones</code> contains the epoch numbers in which the learning rate is dropped.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Exponential(lr, change_rate)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Exponentially decreasing learning rate</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>See also <a href="https://en.wikipedia.org/wiki/Learning_rate" class="bare">https://en.wikipedia.org/wiki/Learning_rate</a>.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>--loss &lt;value&gt;</code>
The loss function used for training the multilayer perceptron. The following loss functions are supported:</p>
</li>
</ul>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Specification</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>SquaredError</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Squared error loss.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>CrossEntropy</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Cross entropy loss (N.B. prone to numerical problems!)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>LogisticCrossEntropy</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Logistic cross entropy loss.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>SoftmaxCrossEntropy</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Softmax cross entropy loss. Matches <code>CrossEntropy</code> of PyTorch. Suitable for classification experiments.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>NegativeLogLikelihood</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Negative log likelihood loss.</p></td>
</tr>
</tbody>
</table>
<div class="ulist">
<ul>
<li>
<p><code>--load-weights &lt;value&gt;</code>
Load weights and biases from a dictionary in NumPy <code>.npz</code> format.
The weight matrices should be stored with keys <code>W1,W2,&#8230;&#8203;</code> and the bias vectors with keys <code>b1,b2,&#8230;&#8203;</code>.
See also
<a href="https://numpy.org/doc/stable/reference/generated/numpy.lib.format.html">numpy.lib.format</a>.</p>
</li>
<li>
<p><code>--save-weights &lt;value&gt;</code>
Save weights and biases to a dictionary in NumPy <code>.npz</code> format.
The weight matrices are stored with keys <code>W1,W2,&#8230;&#8203;</code> and the bias vectors with keys <code>b1,b2,&#8230;&#8203;</code>.
See also
<a href="https://numpy.org/doc/stable/reference/generated/numpy.lib.format.html">numpy.lib.format</a>.</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_dataset_options">Dataset options</h5>
<div class="ulist">
<ul>
<li>
<p><code>--cifar10 &lt;directory&gt;</code>
Specify the directory where the binary version of the
<a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a> dataset is stored. This is a directory with subdirectory <code>cifar-10-batches-bin</code> for the C++ version or <code>cifar-10-batches-py</code> for the Python version of the dataset.</p>
</li>
<li>
<p><code>--mnist &lt;directory&gt;</code>
Specify the directory where the <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a> dataset is stored.
It should be stored in a file named <code>mnist.npz</code>, that can be downloaded <a href="https://s3.amazonaws.com/img-datasets/mnist.npz">here</a>.</p>
</li>
<li>
<p><code>--load-data &lt;value&gt;</code>
Load the dataset from a file in NumPy <code>.npz</code> format. See</p>
</li>
<li>
<p><code>--save-data &lt;value&gt;</code>
Save the dataset to a file in NumPy <code>.npz</code> format. See</p>
</li>
<li>
<p><code>--normalize</code>
Normalize the dataset.</p>
</li>
<li>
<p><code>--preprocessed &lt;directory&gt;</code>
A directory containing datasets named <code>epoch0.npz</code>, <code>epoch1.npz</code>, &#8230;&#8203; See <a href="#io">I/O</a> for information about the <code>.npz</code> format. This can for example be used to precompute augmented datasets. A script <a href="../python/tools/generate_cifar10_augmented_datasets.py">generate_cifar10_augmented_datasets.py</a> is available for creating augmented CIFAR-10 datasets.</p>
</li>
<li>
<p><code>--generate-data &lt;name&gt;</code>
Specify a synthetic dataset that is generated on the fly. The following datasets are supported:</p>
</li>
</ul>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Specification</th>
<th class="tableblock halign-left valign-top">Description</th>
<th class="tableblock halign-left valign-top">Features</th>
<th class="tableblock halign-left valign-top">Classes</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>checkerboard</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A checkerboard pattern, see also <a href="https://kaifishr.github.io/2021/01/14/micro-mlp.html#checkerboard">checkerboard</a>.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>mini</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A dataset with random values.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">3</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2</p></td>
</tr>
</tbody>
</table>
<div class="ulist">
<ul>
<li>
<p><code>--dataset-size &lt;value&gt;</code>
The size of the generated dataset (default: 1000).
<code>--save-weights</code> for information about the format.
<code>--load-weights</code> for information about the format.</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_miscellaneous_options">Miscellaneous options</h5>
<div class="ulist">
<ul>
<li>
<p><code>--info</code>
Print detailed information about the multilayer perceptron.</p>
</li>
<li>
<p><code>--timer</code>
Print timer messages. The following values are supported:</p>
</li>
</ul>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Value</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>disabled</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">No timing information is displayed</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>brief</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">At the end, a report with accumulated timing measurements will be displayed</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>full</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">In addition, individual timing measurements will be displayed</p></td>
</tr>
</tbody>
</table>
<div class="ulist">
<ul>
<li>
<p><code>--precision &lt;value&gt;</code>
The precision used for printing matrix elements.</p>
</li>
<li>
<p><code>--edgeitems &lt;value&gt;</code>
The edgeitems used for printing matrices. This sets the number of border rows and columns that are printed.</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_pruning_and_growing_options">Pruning and growing options</h5>
<div class="ulist">
<ul>
<li>
<p><code>--prune &lt;strategy&gt;</code>
The strategy used for pruning sparse weight matrices. The following strategies are supported:</p>
</li>
</ul>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Specification</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Magnitude(&lt;drop_fraction&gt;)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Magnitude based pruning. A fraction of the weights with the smallest absolute value is pruned.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>SET(&lt;drop_fraction&gt;)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">SET pruning. Positive and negative weights are treated separately. Both a fraction of the positive and a fraction of the negative weights is pruned.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Threshold(&lt;threshold&gt;)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Weights with absolute value below the given threshold are pruned.</p></td>
</tr>
</tbody>
</table>
<div class="ulist">
<ul>
<li>
<p><code>--grow &lt;strategy&gt;</code>
The strategy used for growing in sparse weight matrices. The following strategies are supported:</p>
</li>
</ul>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Specification</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Random</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Weights are added at random positions (outside the support of the sparse matrix).</p></td>
</tr>
</tbody>
</table>
<div class="ulist">
<ul>
<li>
<p><code>--grow-weights &lt;value&gt;</code>
The weight generation function used for growing weights.
See <code>--layer-weights</code> for supported values. The default value is <code>Xavier</code>.</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_computation_options">Computation options</h5>
<div class="ulist">
<ul>
<li>
<p><code>--computation &lt;value&gt;</code>
The computation mode that is used for backpropagation. This is used for performance measurements. The following computation modes are available:</p>
</li>
</ul>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Specification</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>eigen</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">All computations are done using the Eigen library. Note that by setting the flag <code>EIGEN_USE_MKL_ALL</code> Eigen will attempt to use MKL library calls.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>mkl</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Some computations are implemented using MKL functions.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>blas</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Some computations are implemented using BLAS functions.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>sycl</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Some computations are implemented using SYCL functions.</p></td>
</tr>
</tbody>
</table>
<div class="ulist">
<ul>
<li>
<p><code>--clip &lt;value&gt;</code>
A threshold value used to set small elements of weight matrices to zero.</p>
</li>
<li>
<p><code>--threads &lt;value&gt;</code>
The number of threads used by the MKL and OMP libraries.</p>
</li>
<li>
<p><code>--gradient-step &lt;value&gt;</code>
If this value is set, gradient checks are performed with the given step size. This is very slow, and should only be used for debugging.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_the_tool_mkl">The tool mkl</h3>
<div class="paragraph">
<p>The tool <code>mkl</code> is used for benchmarking sparse and dense matrix products. An example of running the <code>mkl</code> tool is</p>
</div>
<div class="listingblock small-code">
<div class="content">
<pre class="rouge highlight"><code data-lang="bash">../install/bin/mkl <span class="nt">--arows</span><span class="o">=</span>1000 <span class="nt">--acols</span><span class="o">=</span>1000 <span class="nt">--brows</span><span class="o">=</span>1000 <span class="nt">--threads</span><span class="o">=</span>12 <span class="nt">--algorithm</span><span class="o">=</span>sdd <span class="nt">--repetitions</span><span class="o">=</span>3 <span class="nt">--densities</span><span class="o">=</span><span class="s2">"0.5,0.2,0.1,0.05"</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>This will use various algorithms to calculate the product <code>A = B * C</code> with <code>A</code> a sparse matrix and <code>B</code> and <code>C</code> dense matrices.</p>
</div>
<div class="paragraph">
<p>The output may look like this</p>
</div>
<div class="listingblock small-code">
<div class="content">
<pre>--- testing A = B * C (sdd_product) ---
A = 1000x1000 sparse
B = 1000x1000 dense  layout=column-major
C = 1000x1000 dense  layout=column-major

density(A) = 0.5
 0.01147s ddd_product A=column-major, B=column-major, C=column-major
 0.00793s ddd_product A=column-major, B=column-major, C=column-major
 0.00854s ddd_product A=column-major, B=column-major, C=column-major
 0.04049s sdd_product(batchsize=5, density(A)=0.499599, B=column-major, C=column-major)
 0.01998s sdd_product(batchsize=5, density(A)=0.499599, B=column-major, C=column-major)
 0.01178s sdd_product(batchsize=5, density(A)=0.499599, B=column-major, C=column-major)
 0.01114s sdd_product(batchsize=10, density(A)=0.499599, B=column-major, C=column-major)
 0.01099s sdd_product(batchsize=10, density(A)=0.499599, B=column-major, C=column-major)
 0.00666s sdd_product(batchsize=10, density(A)=0.499599, B=column-major, C=column-major)
 0.00375s sdd_product(batchsize=100, density(A)=0.499599, B=column-major, C=column-major)
 0.00734s sdd_product(batchsize=100, density(A)=0.499599, B=column-major, C=column-major)
 0.00332s sdd_product(batchsize=100, density(A)=0.499599, B=column-major, C=column-major)
 0.20097s sdd_product_forloop_eigen(density(A)=0.499599, B=column-major, C=column-major)
 0.19891s sdd_product_forloop_eigen(density(A)=0.499599, B=column-major, C=column-major)
 0.19893s sdd_product_forloop_eigen(density(A)=0.499599, B=column-major, C=column-major)
 0.23286s sdd_product_forloop_mkl(density(A)=0.499599, B=column-major, C=column-major)
 0.23298s sdd_product_forloop_mkl(density(A)=0.499599, B=column-major, C=column-major)
 0.23281s sdd_product_forloop_mkl(density(A)=0.499599, B=column-major, C=column-major)</pre>
</div>
</div>
<div class="paragraph">
<p>Note that the very first invocation of an MKL function can be slow.</p>
</div>
</div>
<div class="sect2">
<h3 id="_the_tool_inspect_npz">The tool inspect_npz</h3>
<div class="paragraph">
<p>The tool <code>inspect_npz</code> is a simple tool to show the contents of a file in NumPy NPZ format. The tool <code>mlp</code> uses this format to load and save datasets, and to load and save weight matrices + bias vectors of linear layers. The output may look like this:</p>
</div>
<div class="listingblock small-code">
<div class="content">
<pre>W1 (1024x3072) norm = 0.03827324
   [-0.00850412,  0.00766624, -0.00379110,  ..., -0.02755435,  0.00842837,  0.00725122]
   [ 0.03012662, -0.01122476,  0.03765349,  ...,  0.02167689, -0.03734717, -0.01376905]
   [-0.03415587, -0.00498827,  0.00635345,  ..., -0.03036389, -0.01967963,  0.03339641]
   ...,
   [ 0.02993325, -0.00795984,  0.00388659,  ...,  0.01343446, -0.01625269,  0.00398590]
   [ 0.03800971, -0.01185982, -0.00944855,  ...,  0.02083720, -0.00217844,  0.02398606]
   [-0.00879488, -0.01937520, -0.02830209,  ...,  0.03606736, -0.01065827,  0.03293588]
b1= (1024)
   [-0.01735129, -0.01381215,  0.01708755,  ..., -0.01117092, -0.00264273, -0.00976263]
W2 (512x1024) norm = 0.06249978
   [-0.02440289,  0.01362467,  0.03782336,  ...,  0.01342138, -0.01060697, -0.05055390]
   [ 0.06187645, -0.00854158,  0.02849235,  ...,  0.05861567,  0.00708143, -0.06170959]
   [-0.00756755,  0.04718670, -0.02303848,  ...,  0.01513476,  0.00205931,  0.05441900]
   ...,
   [-0.04223771,  0.00852190, -0.00465803,  ...,  0.03600422,  0.00484904, -0.02281546]
   [ 0.03211500, -0.02740303, -0.04652309,  ...,  0.00307061,  0.02427530, -0.02245107]
   [ 0.05210501, -0.00423148, -0.00633851,  ...,  0.02453317,  0.02723335,  0.03589169]
b2= (512)
   [-0.01871627,  0.01150464, -0.01767523,  ..., -0.00220927, -0.01791467, -0.02616516]
W3 (10x512) norm = 0.10718583
   [-0.03256247, -0.09669271, -0.06564181,  ...,  0.00394586, -0.02191557,  0.08828022]
   [-0.09986399, -0.03712691,  0.04332626,  ..., -0.02475236, -0.07359495, -0.09421349]
   [-0.03308030,  0.01280271,  0.09341474,  ..., -0.03470980, -0.03936023,  0.02204999]
   ...,
   [-0.10063093, -0.04294113, -0.04938528,  ...,  0.08151620, -0.00991420,  0.09686699]
   [ 0.04347997, -0.08046009,  0.02828473,  ...,  0.06899156, -0.08314995,  0.07181197]
   [ 0.00575207, -0.06347645, -0.07257712,  ..., -0.00293436, -0.00266003, -0.08468610]
b3= (10)
   [-0.02117447, -0.00115431, -0.03672279,  ..., -0.02902718, -0.02759255,  0.03007624]</pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_overview_of_the_code">Overview of the code</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This section gives an overview of the C++ code in the
<em>Nerva-Colwise C++ Library</em>, and some information that is needed for understanding the code.</p>
</div>
<div class="sect2">
<h3 id="_number_type">Number type</h3>
<div class="paragraph">
<p>The <em>Nerva-Colwise C++ Library</em> uses a type called <code>scalar</code> as its number type. By default, it is defined as a 32-bit float. It is possible to change this by defining the symbol <code>NERVA_USE_DOUBLE</code>, in which case 64 bit doubles are used. The corresponding code is</p>
</div>
<div class="listingblock small-code">
<div class="content">
<pre class="rouge highlight"><code data-lang="cpp"><span class="cp">#ifdef NERVA_USE_DOUBLE
</span><span class="k">using</span> <span class="n">scalar</span> <span class="o">=</span> <span class="kt">double</span><span class="p">;</span>
<span class="cp">#else
</span><span class="k">using</span> <span class="n">scalar</span> <span class="o">=</span> <span class="kt">float</span><span class="p">;</span>
<span class="cp">#endif</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>A more generic approach would be to add a template argument for the number type to most classes and functions. This has been tried in the past, but since it had a negative impact on the readability of the code, it was later removed.</p>
</div>
</div>
<div class="sect2">
<h3 id="_header_files">Header files</h3>
<div class="paragraph">
<p>The most important header files in are given in the table below.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Header file</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><a href="https://github.com/wiegerw/nerva-colwise/blob/main/include/nerva/neural_networks/multilayer_perceptron.h">multilayer_perceptron.h</a></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A multilayer perceptron class.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><a href="https://github.com/wiegerw/nerva-colwise/blob/main/include/nerva/neural_networks/layers.h">layers.h</a></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Neural network layers.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><a href="https://github.com/wiegerw/nerva-colwise/blob/main/include/nerva/neural_networks/activation_functions.h">activation_functions.h</a></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Activation functions.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><a href="https://github.com/wiegerw/nerva-colwise/blob/main/include/nerva/neural_networks/loss_functions.h">loss_functions.h</a></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Loss functions.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><a href="https://github.com/wiegerw/nerva-colwise/blob/main/include/nerva/neural_networks/weights.h">weights.h</a></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Weight initialization functions.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><a href="https://github.com/wiegerw/nerva-colwise/blob/main/include/nerva/neural_networks/optimizers.h">optimizers.h</a></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Optimizer functions, for updating neural network parameters using their gradients.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><a href="https://github.com/wiegerw/nerva-colwise/blob/main/include/nerva/neural_networks/learning_rate_schedulers.h">learning_rate_schedulers.h</a></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Learning rate schedulers, for updating the learning rate during training.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><a href="https://github.com/wiegerw/nerva-colwise/blob/main/include/nerva/neural_networks/training.h">training.h</a></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A stochastic gradient descent algorithm.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><a href="https://github.com/wiegerw/nerva-colwise/blob/main/include/nerva/neural_networks/prune.h">prune.h</a></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Algorithms for pruning sparse weight matrices. This is used for dynamic sparse training.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><a href="https://github.com/wiegerw/nerva-colwise/blob/main/include/nerva/neural_networks/grow.h">grow.h</a></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Algorithms for (re-)growing sparse weights. This is used for dynamic sparse training.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="_classes">Classes</h3>
<div class="sect3">
<h4 id="_class_multilayer_perceptron">Class multilayer_perceptron</h4>
<div class="paragraph">
<p>A multilayer perceptron (MLP) is modeled using the class <code>multilayer_perceptron</code>. It contains a list of layers, and has member functions <code>feedforward</code>, <code>backpropagate</code> and <code>optimize</code> that can be used for training the neural network. Constructing an MLP can be done manually, as is illustrated in the tests:</p>
</div>
<div class="listingblock small-code">
<div class="content">
<pre class="rouge highlight"><code data-lang="cpp"><span class="kt">void</span> <span class="nf">construct_mlp</span><span class="p">(</span><span class="n">multilayer_perceptron</span><span class="o">&amp;</span> <span class="n">M</span><span class="p">,</span>
                   <span class="k">const</span> <span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&amp;</span> <span class="n">W1</span><span class="p">,</span>
                   <span class="k">const</span> <span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&amp;</span> <span class="n">b1</span><span class="p">,</span>
                   <span class="k">const</span> <span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&amp;</span> <span class="n">W2</span><span class="p">,</span>
                   <span class="k">const</span> <span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&amp;</span> <span class="n">b2</span><span class="p">,</span>
                   <span class="k">const</span> <span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&amp;</span> <span class="n">W3</span><span class="p">,</span>
                   <span class="k">const</span> <span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&amp;</span> <span class="n">b3</span><span class="p">,</span>
                   <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">long</span><span class="o">&gt;&amp;</span> <span class="n">sizes</span><span class="p">,</span>
                   <span class="kt">long</span> <span class="n">N</span>
                  <span class="p">)</span>
<span class="p">{</span>
  <span class="kt">long</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">N</span><span class="p">;</span>

  <span class="k">auto</span> <span class="n">layer1</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">relu_layer</span><span class="o">&lt;</span><span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">batch_size</span><span class="p">);</span>
  <span class="n">M</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">layer1</span><span class="p">);</span>
  <span class="k">auto</span> <span class="n">optimizer_W1</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">gradient_descent_optimizer</span><span class="o">&lt;</span><span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">layer1</span><span class="o">-&gt;</span><span class="n">W</span><span class="p">,</span> <span class="n">layer1</span><span class="o">-&gt;</span><span class="n">DW</span><span class="p">);</span>
  <span class="k">auto</span> <span class="n">optimizer_b1</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">gradient_descent_optimizer</span><span class="o">&lt;</span><span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">layer1</span><span class="o">-&gt;</span><span class="n">b</span><span class="p">,</span> <span class="n">layer1</span><span class="o">-&gt;</span><span class="n">Db</span><span class="p">);</span>
  <span class="n">layer1</span><span class="o">-&gt;</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">make_composite_optimizer</span><span class="p">(</span><span class="n">optimizer_W1</span><span class="p">,</span> <span class="n">optimizer_b1</span><span class="p">);</span>
  <span class="n">layer1</span><span class="o">-&gt;</span><span class="n">W</span> <span class="o">=</span> <span class="n">W1</span><span class="p">;</span>
  <span class="n">layer1</span><span class="o">-&gt;</span><span class="n">b</span> <span class="o">=</span> <span class="n">b1</span><span class="p">;</span>

  <span class="k">auto</span> <span class="n">layer2</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">relu_layer</span><span class="o">&lt;</span><span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">batch_size</span><span class="p">);</span>
  <span class="n">M</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">layer2</span><span class="p">);</span>
  <span class="k">auto</span> <span class="n">optimizer_W2</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">gradient_descent_optimizer</span><span class="o">&lt;</span><span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">layer2</span><span class="o">-&gt;</span><span class="n">W</span><span class="p">,</span> <span class="n">layer2</span><span class="o">-&gt;</span><span class="n">DW</span><span class="p">);</span>
  <span class="k">auto</span> <span class="n">optimizer_b2</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">gradient_descent_optimizer</span><span class="o">&lt;</span><span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">layer2</span><span class="o">-&gt;</span><span class="n">b</span><span class="p">,</span> <span class="n">layer2</span><span class="o">-&gt;</span><span class="n">Db</span><span class="p">);</span>
  <span class="n">layer2</span><span class="o">-&gt;</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">make_composite_optimizer</span><span class="p">(</span><span class="n">optimizer_W2</span><span class="p">,</span> <span class="n">optimizer_b2</span><span class="p">);</span>
  <span class="n">layer2</span><span class="o">-&gt;</span><span class="n">W</span> <span class="o">=</span> <span class="n">W2</span><span class="p">;</span>
  <span class="n">layer2</span><span class="o">-&gt;</span><span class="n">b</span> <span class="o">=</span> <span class="n">b2</span><span class="p">;</span>

  <span class="k">auto</span> <span class="n">layer3</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">linear_layer</span><span class="o">&lt;</span><span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">sizes</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">batch_size</span><span class="p">);</span>
  <span class="n">M</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">layer3</span><span class="p">);</span>
  <span class="k">auto</span> <span class="n">optimizer_W3</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">gradient_descent_optimizer</span><span class="o">&lt;</span><span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">layer3</span><span class="o">-&gt;</span><span class="n">W</span><span class="p">,</span> <span class="n">layer3</span><span class="o">-&gt;</span><span class="n">DW</span><span class="p">);</span>
  <span class="k">auto</span> <span class="n">optimizer_b3</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">gradient_descent_optimizer</span><span class="o">&lt;</span><span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">layer3</span><span class="o">-&gt;</span><span class="n">b</span><span class="p">,</span> <span class="n">layer3</span><span class="o">-&gt;</span><span class="n">Db</span><span class="p">);</span>
  <span class="n">layer3</span><span class="o">-&gt;</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">make_composite_optimizer</span><span class="p">(</span><span class="n">optimizer_W3</span><span class="p">,</span> <span class="n">optimizer_b3</span><span class="p">);</span>
  <span class="n">layer3</span><span class="o">-&gt;</span><span class="n">W</span> <span class="o">=</span> <span class="n">W3</span><span class="p">;</span>
  <span class="n">layer3</span><span class="o">-&gt;</span><span class="n">b</span> <span class="o">=</span> <span class="n">b3</span><span class="p">;</span>
<span class="p">}</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>This will create an MLP with three linear layers that have weight matrices <code>W1, W2, W3</code> and bias vectors <code>b1, b2, b3</code>. The parameter <code>sizes</code> contains the input and output sizes of the three layers. Note that the layers and the optimizers are stored using smart pointers. This is done to facilitate the Nerva Python interface. Constructing an MLP like this is quite verbose. An easier way to construct MLPs is provided by the function <code>make_layers</code>, that offers a string based interface.</p>
</div>
<div class="listingblock small-code">
<div class="content">
<pre class="rouge highlight"><code data-lang="cpp">  <span class="n">multilayer_perceptron</span> <span class="n">M</span><span class="p">;</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">layer_specifications</span> <span class="o">=</span> <span class="p">{</span><span class="s">"ReLU"</span><span class="p">,</span> <span class="s">"ReLU"</span><span class="p">,</span> <span class="s">"Linear"</span><span class="p">};</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="kt">size_t</span><span class="o">&gt;</span> <span class="n">linear_layer_sizes</span> <span class="o">=</span> <span class="p">{</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">};</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span> <span class="n">linear_layer_densities</span> <span class="o">=</span> <span class="p">{</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">};</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span> <span class="n">linear_layer_dropouts</span> <span class="o">=</span> <span class="p">{</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">};</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">linear_layer_weights</span> <span class="o">=</span> <span class="p">{</span><span class="s">"XavierNormalized"</span><span class="p">,</span> <span class="s">"Xavier"</span><span class="p">,</span> <span class="s">"He"</span><span class="p">};</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">optimizers</span> <span class="o">=</span> <span class="p">{</span><span class="s">"Nesterov(0.9)"</span><span class="p">,</span> <span class="s">"Momentum(0.9)"</span><span class="p">,</span> <span class="s">"GradientDescent"</span><span class="p">};</span>
  <span class="kt">long</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">5</span><span class="p">;</span>
  <span class="n">std</span><span class="o">::</span><span class="n">mt19937</span> <span class="n">rng</span><span class="p">{</span><span class="n">std</span><span class="o">::</span><span class="n">random_device</span><span class="p">{}()};</span>
  <span class="n">M</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">make_layers</span><span class="p">(</span><span class="n">layer_specifications</span><span class="p">,</span>
                         <span class="n">linear_layer_sizes</span><span class="p">,</span>
                         <span class="n">linear_layer_densities</span><span class="p">,</span>
                         <span class="n">linear_layer_dropouts</span><span class="p">,</span>
                         <span class="n">linear_layer_weights</span><span class="p">,</span>
                         <span class="n">optimizers</span><span class="p">,</span>
                         <span class="n">batch_size</span><span class="p">,</span>
                         <span class="n">rng</span><span class="p">);</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Note that the random number generator argument is used for the generation of the weights. See
<a href="#mlp_tool">mlp command line options</a> for an overview of the supported string arguments.</p>
</div>
</div>
<div class="sect3">
<h4 id="_class_neural_network_layer">Class neural_network_layer</h4>
<div class="paragraph">
<p>The class <code>neural_network_layer</code> is the base class of all neural network layers. It has attributes for the input matrix <code>X</code> and the corresponding gradient <code>DX</code>. Usually a layer has some additional parameters that can be learned by training the neural network. The most important member functions of <code>neural_network_layer</code> are given below.</p>
</div>
<div class="listingblock small-code">
<div class="content">
<pre class="rouge highlight"><code data-lang="cpp">  <span class="c1">/// Do a feedforward step given the input `X`, and store the output in `result`.</span>
  <span class="k">virtual</span> <span class="kt">void</span> <span class="n">feedforward</span><span class="p">(</span><span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&amp;</span> <span class="n">result</span><span class="p">)</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

  <span class="c1">/// Do a backpropagate step given the output `Y`, and its gradient `DY`.</span>
  <span class="c1">/// This will calculate the gradient `DX` of the input `X`, and the gradients</span>
  <span class="c1">/// of the layer parameters.</span>
  <span class="k">virtual</span> <span class="kt">void</span> <span class="n">backpropagate</span><span class="p">(</span><span class="k">const</span> <span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&amp;</span> <span class="n">Y</span><span class="p">,</span> <span class="k">const</span> <span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&amp;</span> <span class="n">DY</span><span class="p">)</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

  <span class="c1">/// Update the layer parameters using their gradients.</span>
  <span class="k">virtual</span> <span class="kt">void</span> <span class="n">optimize</span><span class="p">(</span><span class="n">scalar</span> <span class="n">eta</span><span class="p">)</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span></code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_class_loss_function">Class loss_function</h4>
<div class="paragraph">
<p>The class <code>loss_function</code> is the base class of all loss functions. Although a loss function is similar to a layer, the interface is different:</p>
</div>
<div class="listingblock small-code">
<div class="content">
<pre class="rouge highlight"><code data-lang="cpp">  <span class="c1">/// Calculate the loss for output `Y` and target `T`.</span>
  <span class="p">[[</span><span class="n">nodiscard</span><span class="p">]]</span> <span class="k">virtual</span> <span class="n">scalar</span> <span class="n">value</span><span class="p">(</span><span class="k">const</span> <span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&amp;</span> <span class="n">Y</span><span class="p">,</span> <span class="k">const</span> <span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&amp;</span> <span class="n">T</span><span class="p">)</span> <span class="k">const</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

  <span class="c1">/// Calculate the gradient of the loss for output `Y` and target `T`.</span>
  <span class="p">[[</span><span class="n">nodiscard</span><span class="p">]]</span> <span class="k">virtual</span> <span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span> <span class="n">gradient</span><span class="p">(</span><span class="k">const</span> <span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&amp;</span> <span class="n">Y</span><span class="p">,</span> <span class="k">const</span> <span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&amp;</span> <span class="n">T</span><span class="p">)</span> <span class="k">const</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>So instead of the names <code>feedforward</code> and <code>backpropagate</code>, we use <code>value</code> and <code>gradient</code>.</p>
</div>
<div class="paragraph">
<p>There are five loss functions available:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>squared_error_loss</code></p>
</li>
<li>
<p><code>cross_entropy_loss</code></p>
</li>
<li>
<p><code>logistic_cross_entropy_loss</code></p>
</li>
<li>
<p><code>softmax_cross_entropy_loss</code></p>
</li>
<li>
<p><code>negative_log_likelihood_loss</code></p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_activation_functions">Activation functions</h4>
<div class="paragraph">
<p>Currently, there is no common base class for activation functions. For example, the ReLU activation function is implemented like this:</p>
</div>
<div class="listingblock small-code">
<div class="content">
<pre class="rouge highlight"><code data-lang="cpp"><span class="k">struct</span> <span class="nc">relu_activation</span>
<span class="p">{</span>
  <span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="nc">Matrix</span><span class="p">&gt;</span>
  <span class="k">auto</span> <span class="k">operator</span><span class="p">()(</span><span class="k">const</span> <span class="n">Matrix</span><span class="o">&amp;</span> <span class="n">X</span><span class="p">)</span> <span class="k">const</span>
  <span class="p">{</span>
    <span class="k">return</span> <span class="n">Relu</span><span class="p">(</span><span class="n">X</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="nc">Matrix</span><span class="p">&gt;</span>
  <span class="k">auto</span> <span class="nf">gradient</span><span class="p">(</span><span class="k">const</span> <span class="n">Matrix</span><span class="o">&amp;</span> <span class="n">X</span><span class="p">)</span> <span class="k">const</span>
  <span class="p">{</span>
    <span class="k">return</span> <span class="n">Relu_gradient</span><span class="p">(</span><span class="n">X</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="p">[[</span><span class="n">nodiscard</span><span class="p">]]</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">to_string</span><span class="p">()</span> <span class="k">const</span>
  <span class="p">{</span>
    <span class="k">return</span> <span class="s">"ReLU()"</span><span class="p">;</span>
  <span class="p">}</span>
<span class="p">};</span></code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Currently, there are some inconsistencies between the
interfaces of layers, loss functions and activation functions. This may be changed in the future.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_training_a_neural_network">Training a neural network</h3>
<div class="paragraph">
<p>The class <code>stochastic_gradient_descent_algorithm</code> can be used to train a neural network. It takes as input a multilayer perceptron, a dataset, a loss function, a learning rate scheduler, and a struct containing options like the number of epochs. The main loop looks like this:</p>
</div>
<div class="listingblock small-code">
<div class="content">
<pre class="rouge highlight"><code data-lang="cpp"><span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">epoch</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="n">options</span><span class="p">.</span><span class="n">epochs</span><span class="p">;</span> <span class="o">++</span><span class="n">epoch</span><span class="p">)</span>
<span class="p">{</span>
  <span class="n">on_start_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">);</span>

  <span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span> <span class="n">DY</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">options</span><span class="p">.</span><span class="n">batch_size</span><span class="p">);</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">long</span> <span class="n">batch_index</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">batch_index</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">batch_index</span><span class="o">++</span><span class="p">)</span>
  <span class="p">{</span>
    <span class="n">on_start_batch</span><span class="p">(</span><span class="n">batch_index</span><span class="p">);</span>

    <span class="n">eigen</span><span class="o">::</span><span class="n">eigen_slice</span> <span class="n">batch</span><span class="p">(</span><span class="n">I</span><span class="p">.</span><span class="n">begin</span><span class="p">()</span> <span class="o">+</span> <span class="n">batch_index</span> <span class="o">*</span> <span class="n">options</span><span class="p">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">options</span><span class="p">.</span><span class="n">batch_size</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">Xtrain</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">Eigen</span><span class="o">::</span><span class="n">indexing</span><span class="o">::</span><span class="n">all</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">T</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">Ttrain</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">Eigen</span><span class="o">::</span><span class="n">indexing</span><span class="o">::</span><span class="n">all</span><span class="p">);</span>

    <span class="n">M</span><span class="p">.</span><span class="n">feedforward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">);</span>
    <span class="n">DY</span> <span class="o">=</span> <span class="n">loss</span><span class="o">-&gt;</span><span class="n">gradient</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="n">options</span><span class="p">.</span><span class="n">batch_size</span><span class="p">;</span>
    <span class="n">M</span><span class="p">.</span><span class="n">backpropagate</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">DY</span><span class="p">);</span>
    <span class="n">M</span><span class="p">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">);</span>

    <span class="n">on_end_batch</span><span class="p">(</span><span class="n">batch_index</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="n">on_end_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">);</span>
<span class="p">}</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>In every epoch, the dataset is divided into <code>K</code> batches. A batch <code>X</code> consists of <code>batch_size</code> examples, with corresponding targets <code>T</code> (i.e. the expected outputs). Each batch goes through the three steps of stochastic gradient descent:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>feedforward:</strong> Given an input batch <code>X</code> and
the neural network parameters <code></code>, compute the
output <code>Y</code>.</p>
</li>
<li>
<p><strong>backpropagation:</strong> Given output <code>Y</code> corresponding to input <code>X</code> and targets <code>T</code>, compute the gradient  <code>DY</code> of <code>Y</code> with respect to the loss function. Then from <code>Y</code> and <code>DY</code>, compute the gradient <code>D</code> of the parameters <code></code>.</p>
</li>
<li>
<p><strong>optimization:</strong> Given the gradient <code>D</code>, update
the parameters <code></code>.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Note that the algorithm uses a number of event functions:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Event</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>on_start_training</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Is called at the start of the training</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>on_end_training</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Is called at the end of the training</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>on_start_epoch</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Is called at the start of each epoch</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>on_end_epoch</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Is called at the end of each epoch</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>on_start_batch</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Is called at the start of each batch</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>on_end_batch</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Is called at the end of each batch</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>The user can respond to these events by deriving from the class <code>stochastic_gradient_descent_algorithm</code>. Typical use cases for these event functions are the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Update the learning rate.</p>
</li>
<li>
<p>Renew dropout masks.</p>
</li>
<li>
<p>Prune and grow sparse weights.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Such operations are typically done after each epoch or after a given number of batches.</p>
</div>
<div id="on_start_epoch" class="paragraph">
<p>An example can be found in the tool <code>mlp</code>:</p>
</div>
<div class="listingblock small-code">
<div class="content">
<pre class="rouge highlight"><code data-lang="cpp">    <span class="kt">void</span> <span class="nf">on_start_epoch</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">epoch</span><span class="p">)</span> <span class="k">override</span>
    <span class="p">{</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">reload_data_directory</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span>
      <span class="p">{</span>
        <span class="n">reload_data</span><span class="p">(</span><span class="n">epoch</span><span class="p">);</span>
      <span class="p">}</span>

      <span class="k">if</span> <span class="p">(</span><span class="n">lr_scheduler</span><span class="p">)</span>
      <span class="p">{</span>
        <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">lr_scheduler</span><span class="o">-&gt;</span><span class="k">operator</span><span class="p">()(</span><span class="n">epoch</span><span class="p">);</span>
      <span class="p">}</span>

      <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
      <span class="p">{</span>
        <span class="n">renew_dropout_masks</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">rng</span><span class="p">);</span>
      <span class="p">}</span>

      <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">regrow_function</span><span class="p">)</span>
      <span class="p">{</span>
        <span class="p">(</span><span class="o">*</span><span class="n">regrow_function</span><span class="p">)(</span><span class="n">M</span><span class="p">);</span>
      <span class="p">}</span>

      <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">options</span><span class="p">.</span><span class="n">clip</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
      <span class="p">{</span>
        <span class="n">M</span><span class="p">.</span><span class="n">clip</span><span class="p">(</span><span class="n">options</span><span class="p">.</span><span class="n">clip</span><span class="p">);</span>
      <span class="p">}</span>
    <span class="p">}</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Five actions take place at the start of every epoch:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A preprocessed dataset is loaded from disk, which is done to avoid the expensive computation of augmented data at every epoch.</p>
</li>
<li>
<p>The learning rate is updated if a learning rate scheduler is set.</p>
</li>
<li>
<p>Dropout masks are renewed.</p>
</li>
<li>
<p>Sparse weight matrices are pruned and regrown if a regrow function is specified.</p>
</li>
<li>
<p>Small weights in the subnormal range are clipped to zero if the <code>clip</code> option is set.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_timers">Timers</h3>
<div class="paragraph">
<p>The <em>Nerva-Colwise C++ Library</em> has two timer classes, defined in the header file <code><a href="https://github.com/wiegerw/nerva-colwise/blob/main/include/nerva/utilities/timer.h">timer.h</a></code>:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">class</th>
<th class="tableblock halign-left valign-top">description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>map_timer</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A timer that can be used for timing different operations.
 Each operation is identified using a name, and for each name all timing results are stored.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>resumable_timer</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">This is a <code>map_timer</code> that can be suspended.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>The <em>Nerva-Colwise C++ Library</em> uses a predefined timer <code>nerva_timer</code> that is defined in the header file <code><a href="https://github.com/wiegerw/nerva-colwise/blob/main/include/nerva/neural_networks/nerva_timer.h">nerva_timer.h</a></code>.
The <code>mlp</code> tool uses this timer to keep track of the time spent on feedforward, backpropagate and optimize calls during training, and optionally of other computations. Each computation is identified with a unique name. If the option <code>--timer=brief</code> is set, the accumulated times of all computations will be displayed:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>--- timing results ---
backpropagate        = 5.6162
batchnorm1           = 0.0895
batchnorm2           = 0.0418
batchnorm3           = 0.1030
batchnorm4           = 0.8833
feedforward          = 1.4613
optimize             = 0.1137
total time           = 8.3089</pre>
</div>
</div>
<div class="paragraph">
<p>For fine-grained measurements two macros <code>NERVA_TIMER_START</code> and <code>NERVA_TIMER_STOP</code> are defined for starting and stopping the timer. An example can be found in the backpropagate call of batch normalization layers:</p>
</div>
<div class="listingblock small-code">
<div class="content">
<pre class="rouge highlight"><code data-lang="cpp">  <span class="kt">void</span> <span class="n">backpropagate</span><span class="p">(</span><span class="k">const</span> <span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&amp;</span> <span class="n">Y</span><span class="p">,</span> <span class="k">const</span> <span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&amp;</span> <span class="n">DY</span><span class="p">)</span> <span class="k">override</span>
  <span class="p">{</span>
    <span class="k">using</span> <span class="n">eigen</span><span class="o">::</span><span class="n">diag</span><span class="p">;</span>
    <span class="k">using</span> <span class="n">eigen</span><span class="o">::</span><span class="n">hadamard</span><span class="p">;</span>
    <span class="k">using</span> <span class="n">eigen</span><span class="o">::</span><span class="n">column_repeat</span><span class="p">;</span>
    <span class="k">using</span> <span class="n">eigen</span><span class="o">::</span><span class="n">rows_sum</span><span class="p">;</span>
    <span class="k">using</span> <span class="n">eigen</span><span class="o">::</span><span class="n">identity</span><span class="p">;</span>
    <span class="k">using</span> <span class="n">eigen</span><span class="o">::</span><span class="n">ones</span><span class="p">;</span>
    <span class="k">using</span> <span class="n">eigen</span><span class="o">::</span><span class="n">inv_sqrt</span><span class="p">;</span>
    <span class="k">auto</span> <span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">cols</span><span class="p">();</span>

    <span class="n">NERVA_TIMER_START</span><span class="p">(</span><span class="s">"batchnorm1"</span><span class="p">)</span>
    <span class="n">DZ</span> <span class="o">=</span> <span class="n">hadamard</span><span class="p">(</span><span class="n">column_repeat</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">DY</span><span class="p">);</span>
    <span class="n">NERVA_TIMER_STOP</span><span class="p">(</span><span class="s">"batchnorm1"</span><span class="p">)</span>

    <span class="n">NERVA_TIMER_START</span><span class="p">(</span><span class="s">"batchnorm2"</span><span class="p">)</span>
    <span class="n">Dbeta</span> <span class="o">=</span> <span class="n">rows_sum</span><span class="p">(</span><span class="n">DY</span><span class="p">);</span>
    <span class="n">NERVA_TIMER_STOP</span><span class="p">(</span><span class="s">"batchnorm2"</span><span class="p">)</span>

    <span class="n">NERVA_TIMER_START</span><span class="p">(</span><span class="s">"batchnorm3"</span><span class="p">)</span>
    <span class="n">Dgamma</span> <span class="o">=</span> <span class="n">rows_sum</span><span class="p">(</span><span class="n">hadamard</span><span class="p">(</span><span class="n">DY</span><span class="p">,</span> <span class="n">Z</span><span class="p">));</span>
    <span class="n">NERVA_TIMER_STOP</span><span class="p">(</span><span class="s">"batchnorm3"</span><span class="p">)</span>

    <span class="n">NERVA_TIMER_START</span><span class="p">(</span><span class="s">"batchnorm4"</span><span class="p">)</span>
    <span class="n">DX</span> <span class="o">=</span> <span class="n">hadamard</span><span class="p">(</span><span class="n">column_repeat</span><span class="p">(</span><span class="n">inv_sqrt_Sigma</span> <span class="o">/</span> <span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">hadamard</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">column_repeat</span><span class="p">(</span><span class="o">-</span><span class="n">diag</span><span class="p">(</span><span class="n">DZ</span> <span class="o">*</span> <span class="n">Z</span><span class="p">.</span><span class="n">transpose</span><span class="p">()),</span> <span class="n">N</span><span class="p">))</span> <span class="o">+</span> <span class="n">DZ</span> <span class="o">*</span> <span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="n">identity</span><span class="o">&lt;</span><span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&gt;</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="o">-</span> <span class="n">ones</span><span class="o">&lt;</span><span class="n">eigen</span><span class="o">::</span><span class="n">matrix</span><span class="o">&gt;</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">)));</span>
    <span class="n">NERVA_TIMER_STOP</span><span class="p">(</span><span class="s">"batchnorm4"</span><span class="p">)</span>
  <span class="p">}</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>To avoid any overhead, these macros can be disabled by defining the symbol <code>NERVA_DISABLE_TIMER</code>. If the option <code>--timer=full</code> is set, all individual timings will be displayed:</p>
</div>
<div class="listingblock small-code">
<div class="content">
<pre>    feedforward-1    0.001753s
     batchnorm1-1    0.000096s
     batchnorm2-1    0.000043s
     batchnorm3-1    0.000099s
     batchnorm4-1    0.001125s
  backpropagate-1    0.006895s
       optimize-1    0.000184s
    feedforward-2    0.001773s
     batchnorm1-2    0.000117s
     batchnorm2-2    0.000051s
     batchnorm3-2    0.000124s
     batchnorm4-2    0.001280s
  backpropagate-2    0.006300s
       optimize-2    0.000115s
    feedforward-3    0.001471s
     batchnorm1-3    0.000071s
     batchnorm2-3    0.000023s
     batchnorm3-3    0.000088s
     batchnorm4-3    0.000667s</pre>
</div>
</div>
<div class="paragraph">
<p>The calls are numbered, to make it easy to compare different runs. Unsurprisingly, the timing output shows that the computation labeled <code>batchnorm4</code> takes the majority of time.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_matrix_operations">Matrix operations</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The most important part of the implementation of neural networks consists of matrix operations. In the implementation of activation functions, loss functions and neural network layers, many different matrix operations are needed. In Nerva a structured approach is followed to implement these components. All equations are expressed in terms of the matrix operations in the table below.</p>
</div>
<table id="table_matrix_operations" class="tableblock frame-all grid-all stretch">
<caption class="title">Table 1. matrix operations</caption>
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Operation</th>
<th class="tableblock halign-left valign-top">Code</th>
<th class="tableblock halign-left valign-top">Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(0_{m}\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>zeros(m)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(m \times 1\) column vector with elements equal to 0</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(0_{mn}\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>zeros(m, n)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(m \times n\) matrix with elements equal to 0</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(1_{m}\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>ones(m)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(m \times 1\) column vector with elements equal to 1</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(1_{mn}\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>ones(m, n)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(m \times n\) matrix with elements equal to 1</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(\mathbb{I}_n\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>identity(n)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(n \times n\) identity matrix</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(X^\top\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>X.transpose()</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">transposition</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(cX\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>c * X</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">scalar multiplication, \(c \in \mathbb{R}\)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(X + Y\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>X + Y</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">addition</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(X - Y\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>X - Y</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">subtraction</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(X \cdot Z\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>X * Z</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">matrix multiplication, also denoted as \(XZ\)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(x^\top y~\) or \(~x y^\top\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>dot(x,y)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">dot product, \(x,y \in \mathbb{R}^{m \times 1}\) or \(x,y \in \mathbb{R}^{1 \times n}\)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(X \odot Y\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>hadamard(X,Y)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">element-wise product of \(X\) and \(Y\)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(\mathsf{diag}(X)\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>diag(X)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">column vector that contains the diagonal of \(X\)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(\mathsf{Diag}(x)\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Diag(x)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">diagonal matrix with \(x\) as diagonal, \(x \in \mathbb{R}^{1 \times n}\)  or \(x \in \mathbb{R}^{m \times 1}\)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(1_m^\top \cdot X \cdot 1_n\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>elements_sum(X)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">sum of the elements of \(X\)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(x \cdot 1_n^\top\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>column_repeat(x, n)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(n\) copies of column vector \(x \in \mathbb{R}^{m \times 1}\)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(1_m \cdot x\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>row_repeat(x, m)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(m\) copies of row vector \(x \in \mathbb{R}^{1 \times n}\)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(1_m^\top \cdot X\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>columns_sum(X)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(1 \times n\) row vector with sums of the columns of \(X\)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(X \cdot 1_n\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>rows_sum(X)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(m \times 1\) column vector with sums of the rows of \(X\)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(\max(X)_{col}\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>columns_max(X)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(1 \times n\) row vector with maximum values of the columns of \(X\)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(\max(X)_{row}\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>rows_max(X)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(m \times 1\) column vector with maximum values of the rows of \(X\)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\((1_m^\top \cdot X) / n\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>columns_mean(X)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(1 \times n\) row vector with mean values of the columns of \(X\)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\((X \cdot 1_n) / m\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>rows_mean(X)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(m \times 1\) column vector with mean values of the rows of \(X\)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(f(X)\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>apply(f, X)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">element-wise application of \(f: \mathbb{R} \rightarrow \mathbb{R}\) to \(X\)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(e^X\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>exp(X)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">element-wise application of \(f: x \rightarrow e^x\) to \(X\)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(\log(X)\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>log(X)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">element-wise application of the natural logarithm \(f: x \rightarrow \ln(x)\) to \(X\)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(1 / X\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>reciprocal(X)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">element-wise application of \(f: x \rightarrow 1/x\) to \(X\)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(\sqrt{X}\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>sqrt(X)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">element-wise application of \(f: x \rightarrow \sqrt{x}\) to \(X\)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(X^{-1/2}\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>inv_sqrt(X)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">element-wise application of \(f: x \rightarrow x^{-1/2}\) to \(X\)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(\log(\sigma(X))\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>log_sigmoid(X)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">element-wise application of \(f: x \rightarrow \log(\sigma(x))\) to \(X\)</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>Using this table leads to concise and uniform code. For example, the backpropagation implementation of a softmax layer looks like this:</p>
</div>
<div class="listingblock small-code">
<div class="content">
<pre class="rouge highlight"><code data-lang="cpp">        <span class="n">DZ</span> <span class="o">=</span> <span class="n">hadamard</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">DY</span> <span class="o">-</span> <span class="n">row_repeat</span><span class="p">(</span><span class="n">diag</span><span class="p">(</span><span class="n">Y</span><span class="p">.</span><span class="n">transpose</span><span class="p">()</span> <span class="o">*</span> <span class="n">DY</span><span class="p">).</span><span class="n">transpose</span><span class="p">(),</span> <span class="n">K</span><span class="p">));</span>
        <span class="n">DW</span> <span class="o">=</span> <span class="n">DZ</span> <span class="o">*</span> <span class="n">X</span><span class="p">.</span><span class="n">transpose</span><span class="p">();</span>
        <span class="n">Db</span> <span class="o">=</span> <span class="n">rows_sum</span><span class="p">(</span><span class="n">DZ</span><span class="p">);</span>
        <span class="n">DX</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="n">transpose</span><span class="p">()</span> <span class="o">*</span> <span class="n">DZ</span><span class="p">;</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>See the specification document <a href="https://wiegerw.github.io/nerva-colwise/pdf/nerva-library-specifications.pdf">Nerva library specifications</a> for an overview of how these matrix operations are used.</p>
</div>
<div class="sect2">
<h3 id="_eigen_library">Eigen library</h3>
<div class="paragraph">
<p>The <em>Nerva-Colwise C++ Library</em> uses the <a href="https://eigen.tuxfamily.org/">Eigen library</a> for representing matrices. The matrix operations in table <a href="#table_matrix_operations">matrix operations</a> have been implemented using Eigen, see the file <a href="https://github.com/wiegerw/nerva-colwise/blob/main/include/nerva/neural_networks/matrix_operations.h">matrix_operations.h</a>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_mkl_library">MKL library</h3>
<div class="paragraph">
<p>Using the Eigen library alone is not sufficient for obtaining high performance. Therefore, the <em>Nerva-Colwise C++ Library</em> uses the <a href="https://en.wikipedia.org/wiki/Math_Kernel_Library">Intel Math Kernel library (MKL)</a> as a backend. The Eigen library supports MKL by means of the compiler flag <code>EIGEN_USE_MKL_ALL</code>, see also <a href="https://eigen.tuxfamily.org/dox/TopicUsingIntelMKL.html">TopicUsingIntelMKL.html</a>.
Note that the MKL library is included in the <a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit-download.html">Intel oneAPI base toolkit</a>.</p>
</div>
<div class="paragraph">
<p>The MKL library supports a number of highly efficient, but extremely low-level interfaces for matrix operations. See <a href="https://www.intel.com/content/www/us/en/docs/onemkl/developer-reference-c/2024-2/blas-and-sparse-blas-routines.html">blas-and-sparse-blas-routines.html</a> for an overview. The <em>Nerva-Colwise C++ Library</em> contains matrix classes that hide those low-level details from the user. The table below gives an overview of them.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Header file</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><a href="https://github.com/wiegerw/nerva-colwise/blob/main/include/nerva/neural_networks/mkl_dense_vector.h">mkl_dense_vector.h</a></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A class <code>dense_vector_view</code> that wraps a raw pointer to a vector.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><a href="https://github.com/wiegerw/nerva-colwise/blob/main/include/nerva/neural_networks/mkl_dense_matrix.h">mkl_dense_matrix.h</a></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A class <code>dense_matrix_view</code> that wraps a raw pointer to a matrix, and a class
<code>dense_matrix</code> that stores a dense matrix.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><a href="https://github.com/wiegerw/nerva-colwise/blob/main/include/nerva/neural_networks/mkl_sparse_matrix.h">mkl_sparse_matrix.h</a></code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A class <code>sparse_matrix_csr</code> <sup class="footnote">[<a id="_footnoteref_1" class="footnote" href="#_footnotedef_1" title="View footnote.">1</a>]</sup> that stores a sparse matrix in
<a href="https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)">compressed sparse row</a> (CSR) format.</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
In C++23 the implementation of sparse matrices in CSR format can be greatly simplified, as shown by <a href="https://github.com/BenBrock/matrix-cpos">Ben Brock</a>.
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The sparse CSR matrix functions in the MKL library take an argument of the opaque type <code>sparse_matrix_t</code>. It stores unspecified properties of a sparse matrix. This parameter is poorly documented, and it is unknown when this parameter should be recalculated. For safety reasons, the <em>Nerva-Colwise C++ Library</em> recalculates this parameter after every change to a sparse matrix, which may cause some inefficiencies. See also the function <code>sparse_matrix_csr::construct_csr</code> and
<a href="https://www.intel.com/content/www/us/en/docs/onemkl/developer-reference-c/2024-1/mkl-sparse-create-csr.html">mkl_sparse_?_create_csr</a>.
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="io">I/O</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The default storage format used in the Nerva libraries is the NumPy NPZ format, see <a href="https://numpy.org/doc/stable/reference/generated/numpy.lib.format.html">numpy.lib.format</a>. The reason for choosing this format is portability between C++ and Python implementations. A file in <code>.npz</code> format can be used to store a dictionary of arrays.</p>
</div>
<div class="paragraph">
<p>The <code>mlp</code> tool has options <code>--load-weights</code> and <code>--save-weights</code> for loading and saving the weights and bias vectors of an MLP, and options <code>--load-data</code> and <code>--save-data</code> for loading and saving a dataset in NPZ format. The keys in the dictionary for the weight matrices and bias vectors of linear layers are <code>W1, W2, &#8230;&#8203;</code> and <code>b1, b2, &#8230;&#8203;</code>. The keys for the training data plus targets are <code>Xtrain</code> and <code>Ttrain</code>, while for the test data plus targets we use <code>Xtest</code> and <code>Ttest</code>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_performance">Performance</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This section discusses various aspects that play a role for the performance of a neural network library.</p>
</div>
<div class="sect2">
<h3 id="_mini_batches">Mini-batches</h3>
<div class="paragraph">
<p>In textbooks and tutorials, the training of a neural network is usually explained in terms of individual examples. But in order to achieve high performance, it is absolutely necessary to use mini-batches. On <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Iterative_method">Wikipedia</a> this is explained as follows:</p>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>A compromise between computing the true gradient and the gradient at a single sample is to compute the gradient against more than one training sample (called a "mini-batch") at each step. This can perform significantly better than "true" stochastic gradient descent described, because the code can make use of vectorization libraries rather than computing each step separately.</p>
</div>
</blockquote>
</div>
<div class="paragraph">
<p>To support mini-batches, the <em>Nerva-Colwise C++ Library</em> defines all equations that play a role in the execution of a neural network in matrix form, including the backpropagation equations, see the specification document <a href="https://wiegerw.github.io/nerva-rowwise/pdf/nerva-library-specifications.pdf">Nerva library specifications</a>. For the latter, many neural network frameworks rely on <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">Automatic differentiation</a>, see also <span class="citation">[<a href="#DBLP_journals_jmlr_BaydinPRS17">1</a>]</span>. We use explicit backpropagation equations to implement truly sparse layers and to provide an instructive resource for those studying neural network execution.</p>
</div>
</div>
<div class="sect2">
<h3 id="_matrix_products">Matrix products</h3>
<div class="paragraph">
<p>The performance of training a neural network largely depends on the calculation of matrix products during the backpropagation step of linear layers. In order to do this efficiently, the <a href="https://en.wikipedia.org/wiki/Math_Kernel_Library">Intel Math Kernel library (MKL)</a> is used. Currently, this dependency is hard coded, but there are plans to make this optional. To experiment with other implementations, like SYCL or BLAS, a global setting is used that is discussed in the next section.</p>
</div>
</div>
<div class="sect2">
<h3 id="_nerva_computation_mode">Nerva computation mode</h3>
<div class="paragraph">
<p>In general, the performance of Eigen is very good. But occasionally, the generated code for a matrix expression can be quite poor. Especially in case of backpropagation calculations this can have a huge impact on the performance. The <em>Nerva-Colwise C++ Library</em> uses a global setting <code>NervaComputation</code> to experiment with other implementations. For example, the function <code>softmax_layer::feedforward</code> contains this:</p>
</div>
<div class="listingblock small-code">
<div class="content">
<pre class="rouge highlight"><code data-lang="cpp">      <span class="k">if</span> <span class="p">(</span><span class="n">NervaComputation</span> <span class="o">==</span> <span class="n">computation</span><span class="o">::</span><span class="n">eigen</span><span class="p">)</span>
      <span class="p">{</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">W</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">column_repeat</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">stable_softmax</span><span class="p">()(</span><span class="n">Z</span><span class="p">);</span>
      <span class="p">}</span>
      <span class="k">else</span>
      <span class="p">{</span>
        <span class="n">mkl</span><span class="o">::</span><span class="n">ddd_product</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">);</span>
        <span class="n">Z</span> <span class="o">+=</span> <span class="n">column_repeat</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">stable_softmax</span><span class="p">()(</span><span class="n">Z</span><span class="p">);</span>
      <span class="p">}</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>In this case, the default version <code>computation::eigen</code> turned out to have very poor performance. A direct call to an MKL routine is used to solve this problem. The <code>NervaComputation</code> setting is also used to experiment with BLAS implementations and SYCL implementations. See the file <a href="https://github.com/wiegerw/nerva-colwise/blob/main/include/nerva/neural_networks/optimizers.h">optimizers.h</a> for some examples of that.</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
The command line tool <code>mlp</code> has an option <code>--computation</code> to set the computation mode.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_subnormal_numbers">Subnormal numbers</h3>
<div class="paragraph">
<p>Experiments with sparse neural networks have shown that the performance can be negatively influenced by <a href="https://en.wikipedia.org/wiki/Subnormal_number">subnormal numbers</a>. The example program <code><a href="https://github.com/wiegerw/nerva-colwise/blob/main/examples/subnormal_numbers.cpp">subnormal_numbers.cpp</a></code> illustrates the problem. The table below is the result of the following experiment.
The dot product of two large vectors of floating-point numbers is computed. One vector is filled with random values between 0 and 1, and the other with powers of 10, ranging from <code>1</code> to <code>1e45</code>. For values larger than <code>1e35</code>, the time needed for this calculation is about <code>0.044</code> seconds. For smaller values we end up in the range of subnormal numbers. This causes the runtime to increase more than eight-fold to <code>0.37</code> seconds. In our experiments we observed that when layers with high sparsity are used, it may happen that subnormal values appear in weight matrices, and their amount increases every epoch.</p>
</div>
<div class="listingblock small-code">
<div class="content">
<pre>--- multiplication1 ---
time =   0.044372 | value = 1.0e+00    | sum = -5.49552e+03
time =   0.044567 | value = 1.0e-01    | sum = -5.49572e+02
time =   0.044243 | value = 1.0e-02    | sum = -5.49304e+01
time =   0.044434 | value = 1.0e-03    | sum = -5.49612e+00
time =   0.044253 | value = 1.0e-04    | sum = -5.49862e-01
time =   0.044765 | value = 1.0e-05    | sum = -5.49653e-02
time =   0.044698 | value = 1.0e-06    | sum = -5.49624e-03
time =   0.044683 | value = 1.0e-07    | sum = -5.49642e-04
time =   0.044703 | value = 1.0e-08    | sum = -5.49491e-05
time =   0.044821 | value = 1.0e-09    | sum = -5.49454e-06
time =   0.044705 | value = 1.0e-10    | sum = -5.49557e-07
time =   0.044657 | value = 1.0e-11    | sum = -5.49730e-08
time =   0.045235 | value = 1.0e-12    | sum = -5.49563e-09
time =   0.045120 | value = 1.0e-13    | sum = -5.49706e-10
time =   0.045010 | value = 1.0e-14    | sum = -5.49719e-11
time =   0.044988 | value = 1.0e-15    | sum = -5.49464e-12
time =   0.044943 | value = 1.0e-16    | sum = -5.49629e-13
time =   0.044795 | value = 1.0e-17    | sum = -5.49573e-14
time =   0.044147 | value = 1.0e-18    | sum = -5.49449e-15
time =   0.044166 | value = 1.0e-19    | sum = -5.49589e-16
time =   0.044380 | value = 1.0e-20    | sum = -5.49722e-17
time =   0.044036 | value = 1.0e-21    | sum = -5.49430e-18
time =   0.043405 | value = 1.0e-22    | sum = -5.49577e-19
time =   0.043615 | value = 1.0e-23    | sum = -5.49548e-20
time =   0.043544 | value = 1.0e-24    | sum = -5.49570e-21
time =   0.043547 | value = 1.0e-25    | sum = -5.49694e-22
time =   0.043536 | value = 1.0e-26    | sum = -5.49365e-23
time =   0.043560 | value = 1.0e-27    | sum = -5.49488e-24
time =   0.043500 | value = 1.0e-28    | sum = -5.49657e-25
time =   0.043524 | value = 1.0e-29    | sum = -5.49783e-26
time =   0.044128 | value = 1.0e-30    | sum = -5.49559e-27
time =   0.043585 | value = 1.0e-31    | sum = -5.49745e-28
time =   0.043530 | value = 1.0e-32    | sum = -5.49488e-29
time =   0.043609 | value = 1.0e-33    | sum = -5.49569e-30
time =   0.043805 | value = 1.0e-34    | sum = -5.49446e-31
time =   0.046169 | value = 1.0e-35    | sum = -5.49661e-32
time =   0.070594 | value = 1.0e-36    | sum = -5.49664e-33
time =   0.247938 | value = 1.0e-37    | sum = -5.49684e-34
time =   0.368848 | value = 1.0e-38    | sum = -5.49553e-35
time =   0.369819 | value = 1.0e-39    | sum = -5.49426e-36
time =   0.368434 | value = 1.0e-40    | sum = -5.49607e-37
time =   0.368747 | value = 1.0e-41    | sum = -5.49801e-38
time =   0.369033 | value = 1.0e-42    | sum = -5.50173e-39
time =   0.370241 | value = 9.9e-44    | sum = -5.47762e-40
time =   0.370065 | value = 9.8e-45    | sum = -4.97559e-41
time =   0.370310 | value = 1.4e-45    | sum = -1.44152e-41</pre>
</div>
</div>
<div class="paragraph">
<p>On <a href="https://groups.google.com/g/llvm-dev/c/TDGKHFU4hzE/m/k-LEa3NvBQAJ">Google Groups</a> this problem is discussed. A possible solution is to instruct the compiler to flush subnormal values to zero. But there doesn&#8217;t seem to be a portable way to achieve this. In the <em>Nerva-Colwise C++ Library</em> different solutions have been tried. One of them is to periodically flush weights in the subnormal range to zero using the <code>--clip</code> command line option of the <code>mlp</code> tool.
In <span class="citation">[<a href="#DBLP_journals_actanum_HighamM22">2</a>]</span> the problem of subnormal numbers is discussed.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_sparse_neural_networks">Sparse neural networks</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Sparse neural network layers are often simulated using binary masks, see <span class="citation">[<a href="#DBLP_journals_corr_abs-2102-01732">3</a>]</span>. This is caused by the lack of support for sparse tensors in popular neural network frameworks. Note that PyTorch is currently developing <a href="https://pytorch.org/docs/stable/sparse.html">sparse tensors</a>. The <em>Nerva-Colwise C++ Library</em> supports truly sparse layers, meaning that the weight matrices of sparse layers are stored in a sparse matrix format. Another example of truly sparse layers is given by <span class="citation">[<a href="#DBLP_conf_icml_NikdanPIKA23">4</a>]</span>.</p>
</div>
<div class="sect2">
<h3 id="_sparse_matrices">Sparse matrices</h3>
<div class="paragraph">
<p>Since we are dealing with a programming context, we say that the <em>support</em> of a sparse matrix refers to the set of positions (or indices) in the matrix that are explicitly stored. Elements inside the support can have a non-zero value. Elements outside the support have the value zero by definition.</p>
</div>
<div class="paragraph">
<p>Sparse matrices in the <em>Nerva-Colwise C++ Library</em> are stored in
<a href="https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)">CSR</a> format.
This matrix representation stores arrays of column and row indices to define the support, plus an array of the corresponding values. CSR matrices are <em>unstructured</em> sparse matrices, meaning they have non-zero elements located at arbitrary positions. Alternatively, there are <em>structured</em> sparse matrices, take for example butterfly matrices <span class="citation">[<a href="#DBLP_journals_corr_abs-2405-15013">5</a>]</span>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_sparse_evolutionary_training">Sparse evolutionary training</h3>
<div class="paragraph">
<p>Sparse evolutionary training (SET) is a method for efficiently training sparse neural networks, see e.g. <span class="citation">[<a href="#DBLP_journals_nca_LiuMMPP21">6</a>]</span>. The idea behind this method is to start the training with a random sparse topology, and to periodically prune and regrow some of the weights.</p>
</div>
</div>
<div class="sect2">
<h3 id="_sparse_initialization">Sparse initialization</h3>
<div class="paragraph">
<p>In SET, the sparsity is not divided evenly over the sparse layers. Instead, small layers are assigned a higher density than larger ones.
 In <span class="citation">[<a href="#DBLP_journals_nca_LiuMMPP21">6</a>]</span> formula (3), <a href="https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model">ErdsRnyi graph topology</a>
 is suggested to calculate the densities of the sparse layers given a desired overall density of the sparse layers combined.
In the <em>Nerva-Colwise C++ Library</em> this is implemented in the function <code>compute_sparse_layer_densities</code>, see <code><a href="https://github.com/wiegerw/nerva-colwise/blob/main/include/nerva/neural_networks/layer_algorithms.h">layer_algorithms.h</a></code>. The original Python implementation can be found
<a href="https://github.com/VITA-Group/Random_Pruning/blob/871077f1d10f9bc44941b093fd5ccbc4ec3984fa/CIFAR/sparselearning/core.py#L155">here</a>, along with several other sparse initialization strategies.
In the tool <code>mlp</code> the option <code>--overall-density</code> is used for assigning ErdsRnyi densities to the sparse layers. See <a href="#mlp_output">[mlp_output]</a> for an example of this. The overall density of <code>0.05</code> is converted into densities <code>[0.042382877, 0.06357384, 1.0]</code> for the individual layers.</p>
</div>
</div>
<div class="sect2">
<h3 id="_pruning_weights">Pruning weights</h3>
<div class="paragraph">
<p>Pruning weights is about removing parameters from a neural network, see also <a href="https://en.wikipedia.org/wiki/Pruning_(artificial_neural_network)">Pruning (artificial_neural_network)</a>. In our context removing parameters is about removing elements from the support of a sparse weight matrix. The effect of this is that the values corresponding to these elements are zeroed.</p>
</div>
<div class="sect3">
<h4 id="_threshold_pruning">Threshold pruning</h4>
<div class="paragraph">
<p>In threshold pruning, all weights \(w_{ij}\) with \(|w_{ij}| \leq t\) for a given threshold \(t\) are pruned from a weight matrix \(W\).</p>
</div>
</div>
<div class="sect3">
<h4 id="_magnitude_based_pruning">Magnitude based pruning</h4>
<div class="paragraph">
<p>Magnitude based pruning is special case of threshold pruning. In magnitude based pruning, the threshold \(t\) is computed such that for a given fraction \(\zeta\) of the weights we have \(|w_{ij}| \leq t\). To ensure that the desired fraction of weights is removed, our implementation takes into account that there can be multiple weights with
\(|w_{ij}| = t\).</p>
</div>
</div>
<div class="sect3">
<h4 id="_set_based_pruning">SET based pruning</h4>
<div class="paragraph">
<p>In SET based pruning, magnitude pruning is applied to positive weights and negative weights separately. So a fraction \(\zeta\) of the positive weights and a fraction \(\zeta\) of the negative weights are pruned.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_growing_weights">Growing weights</h3>
<div class="paragraph">
<p>Growing weights is about adding parameters to a neural network. In our context adding parameters is about adding elements to the support of a sparse weight matrix.</p>
</div>
<div class="sect3">
<h4 id="_random_growing">Random growing</h4>
<div class="paragraph">
<p>In random growing, a given number of elements is chosen randomly from the positions outside the support of a weight matrix. These new elements are then added to the support. Since the new elements need to be initialized, a weight initializer needs to be chosen to generate values for them.</p>
</div>
<div class="paragraph">
<p>A specific implementation <code>grow_random</code> for matrices in CSR format has been developed, that uses <a href="https://en.wikipedia.org/wiki/Reservoir_sampling">reservoir sampling</a> to determine the new elements that are added to the support.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_classes_for_pruning_and_growing">Classes for pruning and growing</h3>
<div class="paragraph">
<p>In the <em>Nerva-Colwise C++ Library</em>, the classes <code>prune_function</code> and <code>grow_function</code> are used to represent generic pruning and growing strategies:</p>
</div>
<div class="listingblock small-code">
<div class="content">
<pre class="rouge highlight"><code data-lang="cpp"><span class="k">struct</span> <span class="nc">prune_function</span>
<span class="p">{</span>
  <span class="c1">/// Removes elements from the support of a sparse matrix</span>
  <span class="c1">/// @param W A sparse matrix</span>
  <span class="c1">/// @return The number of elements removed from the support</span>
  <span class="k">virtual</span> <span class="n">std</span><span class="o">::</span><span class="kt">size_t</span> <span class="k">operator</span><span class="p">()(</span><span class="n">mkl</span><span class="o">::</span><span class="n">sparse_matrix_csr</span><span class="o">&lt;</span><span class="n">scalar</span><span class="o">&gt;&amp;</span> <span class="n">W</span><span class="p">)</span> <span class="k">const</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

  <span class="k">virtual</span> <span class="o">~</span><span class="n">prune_function</span><span class="p">()</span> <span class="o">=</span> <span class="k">default</span><span class="p">;</span>
<span class="p">};</span></code></pre>
</div>
</div>
<div class="listingblock small-code">
<div class="content">
<pre class="rouge highlight"><code data-lang="cpp"><span class="k">struct</span> <span class="nc">grow_function</span>
<span class="p">{</span>
  <span class="c1">/// Adds `count` elements to the support of matrix `W`</span>
  <span class="k">virtual</span> <span class="kt">void</span> <span class="k">operator</span><span class="p">()(</span><span class="n">mkl</span><span class="o">::</span><span class="n">sparse_matrix_csr</span><span class="o">&lt;</span><span class="n">scalar</span><span class="o">&gt;&amp;</span> <span class="n">W</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="kt">size_t</span> <span class="n">count</span><span class="p">)</span> <span class="k">const</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

  <span class="k">virtual</span> <span class="o">~</span><span class="n">grow_function</span><span class="p">()</span> <span class="o">=</span> <span class="k">default</span><span class="p">;</span>
<span class="p">};</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>In the command line tool <code>mlp</code> the user can select specific implementations of these prune and grow functions. They are called at the start of each epoch of training via an attribute <code>regrow_function</code> that applies pruning and growing to the sparse layers of an MLP. See also the <a href="#on_start_epoch">[on_start_epoch]</a> event.</p>
</div>
</div>
<div class="sect2">
<h3 id="_experiments_with_sparse_training">Experiments with sparse training</h3>
<div class="paragraph">
<p>In <span class="citation">[<a href="#wesselink2024nervatrulysparseimplementation">7</a>]</span> we report on some of our experiments with sparse neural networks.</p>
</div>
<div class="paragraph">
<p>An example of a dynamic sparse training experiment is</p>
</div>
<div class="listingblock small-code">
<div class="content">
<pre class="rouge highlight"><code data-lang="bash">../install/bin/mlp <span class="se">\</span>
    <span class="nt">--layers</span><span class="o">=</span><span class="s2">"ReLU;ReLU;Linear"</span> <span class="se">\</span>
    <span class="nt">--layer-sizes</span><span class="o">=</span><span class="s2">"3072;1024;1024;10"</span> <span class="se">\</span>
    <span class="nt">--layer-weights</span><span class="o">=</span>Xavier <span class="se">\</span>
    <span class="nt">--optimizers</span><span class="o">=</span><span class="s2">"Nesterov(0.9)"</span> <span class="se">\</span>
    <span class="nt">--loss</span><span class="o">=</span>SoftmaxCrossEntropy <span class="se">\</span>
    <span class="nt">--learning-rate</span><span class="o">=</span>0.01 <span class="se">\</span>
    <span class="nt">--epochs</span><span class="o">=</span>100 <span class="se">\</span>
    <span class="nt">--batch-size</span><span class="o">=</span>100 <span class="se">\</span>
    <span class="nt">--threads</span><span class="o">=</span>12 <span class="se">\</span>
    <span class="nt">--overall-density</span><span class="o">=</span>0.05 <span class="se">\</span>
    <span class="nt">--prune</span><span class="o">=</span><span class="s2">"Magnitude(0.2)"</span> <span class="se">\</span>
    <span class="nt">--grow</span><span class="o">=</span>Random <span class="se">\</span>
    <span class="nt">--cifar10</span><span class="o">=</span>../data <span class="se">\</span>
    <span class="nt">--seed</span><span class="o">=</span>123</code></pre>
</div>
</div>
<div class="paragraph">
<p>At the start of every epoch 20% of the weights is pruned, and the same number of weights is added back at different locations.
The output may look like this:</p>
</div>
<div class="listingblock small-code">
<div class="content">
<pre>=== Nerva c++ model ===
Sparse(input_size=3072, output_size=1024, density=0.042382877, optimizer=Nesterov(0.90000), activation=ReLU())
Sparse(input_size=1024, output_size=1024, density=0.06357384, optimizer=Nesterov(0.90000), activation=ReLU())
Dense(input_size=1024, output_size=10, optimizer=Nesterov(0.90000), activation=NoActivation())
loss = SoftmaxCrossEntropyLoss()
scheduler = ConstantScheduler(lr=0.01)
layer densities: 133325/3145728 (4.238%), 66662/1048576 (6.357%), 10240/10240 (100%)

epoch   0 lr: 0.01000000  loss: 2.30284437  train accuracy: 0.07904000  test accuracy: 0.08060000 time: 0.00000000s
epoch   1 lr: 0.01000000  loss: 2.14723837  train accuracy: 0.21136000  test accuracy: 0.21320000 time: 5.48583113s
pruning + growing 26665/133325 weights
pruning + growing 13332/66662 weights
epoch   2 lr: 0.01000000  loss: 1.91203228  train accuracy: 0.30918000  test accuracy: 0.30900000 time: 5.00460376s
pruning + growing 26665/133325 weights
pruning + growing 13332/66662 weights</pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="extending">Extending the library</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The <em>Nerva-Colwise C++ Library</em> can be extended in several obvious ways, such as adding new layers, activation functions, loss functions, learning rate schedulers and pruning or growing functions. This can be done by inheriting from the appropriate base class and implementing the required virtual functions. The table below provides an overview:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Functionality</th>
<th class="tableblock halign-left valign-top">Base class</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">A layer</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>neural_network_layer</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">An activation function</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>activation_function</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">A loss function</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>loss_function</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">A learning rate scheduler</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>learning_rate_scheduler</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">A pruning function</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>prune_function</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">A growing function</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>grow_function</code></p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>It is recommended to follow the approach advocated in the Nerva libraries. Each implementation should be based on a mathematical specification, as shown in the
<a href="https://wiegerw.github.io/nerva-rowwise/pdf/nerva-library-specifications.pdf">Nerva library specifications</a> document.
After defining the mathematical equations, you can use the table of <a href="#table_matrix_operations">matrix operations</a> to convert the equations into code.</p>
</div>
<div class="paragraph">
<p>Another crucial step is validation and testing. The symbolic mathematics library
<a href="https://docs.sympy.org/latest/index.html">SymPy</a>
can be used to validate the equations.
The <a href="https://github.com/wiegerw/nerva-sympy">nerva-sympy</a> repository contains <a href="https://github.com/wiegerw/nerva-sympy/tree/main/tests">test cases</a> for activation functions, loss functions, layers, and even for the derivation of equations.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_references">References</h2>
<div class="sectionbody">
<div class="paragraph">
<p><a id="DBLP_journals_jmlr_BaydinPRS17"></a>[1] A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind, Automatic Differentiation in Machine Learning: a Survey, <em>J. Mach. Learn. Res.</em>, vol. 18, pp. 153:1153:43, 2017, [Online]. Available: <a href="https://jmlr.org/papers/v18/17-468.html" class="bare">https://jmlr.org/papers/v18/17-468.html</a>.</p>
</div>
<div class="paragraph">
<p><a id="DBLP_journals_actanum_HighamM22"></a>[2] N. J. Higham and T. Mary, Mixed precision algorithms in numerical linear algebra, <em>Acta Numer.</em>, vol. 31, pp. 347414, 2022, [Online]. Available: <a href="https://doi.org/10.1017/S0962492922000022" class="bare">https://doi.org/10.1017/S0962492922000022</a>.</p>
</div>
<div class="paragraph">
<p><a id="DBLP_journals_corr_abs-2102-01732"></a>[3] S. Curci, D. C. Mocanu, and M. Pechenizkiy, Truly Sparse Neural Networks at Scale, <em>CoRR</em>, vol. abs/2102.01732, 2021, [Online]. Available: <a href="https://arxiv.org/abs/2102.01732" class="bare">https://arxiv.org/abs/2102.01732</a>.</p>
</div>
<div class="paragraph">
<p><a id="DBLP_conf_icml_NikdanPIKA23"></a>[4] M. Nikdan, T. Pegolotti, E. Iofinova, E. Kurtic, and D. Alistarh, SparseProp: Efficient Sparse Backpropagation for Faster Training of Neural Networks at the Edge, in <em>International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA</em>, 2023, vol. 202, pp. 2621526227, [Online]. Available: <a href="https://proceedings.mlr.press/v202/nikdan23a.html" class="bare">https://proceedings.mlr.press/v202/nikdan23a.html</a>.</p>
</div>
<div class="paragraph">
<p><a id="DBLP_journals_corr_abs-2405-15013"></a>[5] A. Gonon, L. Zheng, P. Carrivain, and Q.-T. Le, Make Inference Faster: Efficient GPU Memory Management for Butterfly Sparse Matrix Multiplication, <em>CoRR</em>, vol. abs/2405.15013, 2024, [Online]. Available: <a href="https://doi.org/10.48550/arXiv.2405.15013" class="bare">https://doi.org/10.48550/arXiv.2405.15013</a>.</p>
</div>
<div class="paragraph">
<p><a id="DBLP_journals_nca_LiuMMPP21"></a>[6] S. Liu, D. C. Mocanu, A. R. R. Matavalam, Y. Pei, and M. Pechenizkiy, Sparse evolutionary deep learning with over one million artificial neurons on commodity hardware, <em>Neural Comput. Appl.</em>, vol. 33, no. 7, pp. 25892604, 2021, [Online]. Available: <a href="https://doi.org/10.1007/s00521-020-05136-7" class="bare">https://doi.org/10.1007/s00521-020-05136-7</a>.</p>
</div>
<div class="paragraph">
<p><a id="wesselink2024nervatrulysparseimplementation"></a>[7] W. Wesselink, B. Grooten, Q. Xiao, C. de Campos, and M. Pechenizkiy, Nerva: a Truly Sparse Implementation of Neural Networks. 2024, [Online]. Available: <a href="https://arxiv.org/abs/2407.17437" class="bare">https://arxiv.org/abs/2407.17437</a>.</p>
</div>
</div>
</div>
</div>
<div id="footnotes">
<hr>
<div class="footnote" id="_footnotedef_1">
<a href="#_footnoteref_1">1</a>. Note that the name <code>sparse_matrix</code> could not be used due to a conflict with a #define of the same name buried deep inside the MKL library code
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2024-10-04 16:49:04 UTC
</div>
</div>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]],
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },
  TeX: { equationNumbers: { autoNumber: "none" } }
})
MathJax.Hub.Register.StartupHook("AsciiMath Jax Ready", function () {
  MathJax.InputJax.AsciiMath.postfilterHooks.Add(function (data, node) {
    if ((node = data.script.parentNode) && (node = node.parentNode) && node.classList.contains("stemblock")) {
      data.math.root.display = "block"
    }
    return data
  })
})
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
</body>
</html>